{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1490345568000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1490345568000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1490345568000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1490345568000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1490345568000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1490345568000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1490345568000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1490345568000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1490345568000},{"_id":"themes/next/_config.yml","hash":"ff5637e0727e1d010227ae60244e3bd96e8cbf23","modified":1493868012866},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1490345568000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1490345568000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1490345568000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1490345568000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1490345568000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1490345568000},{"_id":"source/_posts/2016-11-11-ubunt16.04下caffe环境安装.md","hash":"280c2e34f76c09ab65dd618b4f91765a35c96d45","modified":1492494339187},{"_id":"source/_posts/2017-02-08-制作自己的图片数据集.md","hash":"d98d9e35b342a540bdfdcf3d61d5fffcbf5f1c68","modified":1492494339191},{"_id":"source/_posts/2017-02-21-用faster-rcnn训练自己的数据集(python版).md","hash":"921c4ee43df65b14f9a254c81fd3d832b9a1b6ac","modified":1492494339191},{"_id":"source/images/avatar.jpg","hash":"e8b541d0375312a4b41917a0214ce15a2c17b100","modified":1437983792273},{"_id":"source/categories/index.md","hash":"fec9762da8850ce37d6ed3b6bbf70fdf6bafa182","modified":1492496129149},{"_id":"source/_posts/2017-02-07-用YOLOv2训练自己的数据集.md","hash":"0221ffaafed85f9b2f01e440d6e238dff85e7414","modified":1492494339187},{"_id":"source/tags/index.md","hash":"10e6159d602e53776ece94393a6cd1a2c0f60522","modified":1492496514462},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1490345568000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1490345568000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1490345568000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1490345568000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1490345568000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1490345568000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1490345568000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1490345568000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1490345568000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1490345568000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1490345568000},{"_id":"themes/next/languages/ru.yml","hash":"7462c3017dae88e5f80ff308db0b95baf960c83f","modified":1490345568000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1490345568000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1490345568000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1490345568000},{"_id":"themes/next/layout/_layout.swig","hash":"909d68b164227fe7601d82e2303bf574eb754172","modified":1490345568000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1490345568000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1490345568000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1490345568000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1490345568000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1490345568000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1490345568000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1490345568000},{"_id":"themes/next/layout/schedule.swig","hash":"234dc8c3b9e276e7811c69011efd5d560519ef19","modified":1490345568000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1490345568000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1490345568000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1490345568000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1490345568000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1490345568000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1490345568000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1490345568000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1490345568000},{"_id":"themes/next/layout/_macro/post.swig","hash":"640b431eccbbd27f10c6781f33db5ea9a6e064de","modified":1490345568000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1490345568000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1490345568000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1490345568000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1490345568000},{"_id":"themes/next/layout/_partials/head.swig","hash":"a0eafe24d1dae30c790ae35612154b3ffbbd5cce","modified":1490345568000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1490345568000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1490345568000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1490345568000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1490345568000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1490345568000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1490345568000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1490345568000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1490345568000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1490345568000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1490345568000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1490345568000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1490345568000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1490345568000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1490345568000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1490345568000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1490345568000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9de352a32865869e7ed6863db271c46db5853e5a","modified":1490345568000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1490345568000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1490345568000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1490345568000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1490345568000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1490345568000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1490345568000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1490345568000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1490345568000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1490345568000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1490345568000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1490345568000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1490345568000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1490345568000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1490345568000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1490345568000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1490345568000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1490345568000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1490345568000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1490345568000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1490345568000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1490345568000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1490345568000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1490345568000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1490345568000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2d1075f4cabcb3956b7b84a8e210f5a66f0a5562","modified":1490345568000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1490345568000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1490345568000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a652f202bd5b30c648c228ab8f0e997eb4928e44","modified":1490345568000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"7240f2e5ec7115f8abbbc4c9ef73d4bed180fdc7","modified":1490345568000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1490345568000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"f4dbd4c896e6510ded8ebe05394c28f8a86e71bf","modified":1490345568000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1490345568000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1490345568000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1490345568000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1490345568000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1490345568000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1490345568000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1490345568000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1490345568000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"28a7f84242ca816a6452a0a79669ca963d824607","modified":1490345568000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1490345568000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1490345568000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1490345568000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1490345568000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1490345568000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1490345568000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1490345568000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1490345568000},{"_id":"themes/next/source/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1490345568000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1490345568000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1490345568000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1490345568000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1490345568000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1490345568000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1490345568000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1490345568000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1490345568000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1490345568000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1490345568000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1490345568000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"1eb34b9c1f6d541605ff23333eeb133e1c4daf17","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1490345568000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1490345568000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1490345568000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1490345568000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1490345568000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1490345568000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1490345568000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"755b04edbbfbdd981a783edb09c9cc34cb79cea7","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"b9a2e76f019a5941191f1263b54aef7b69c48789","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"173490e21bece35a34858e8e534cf86e34561350","modified":1490345568000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1490345568000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1490345568000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1490345568000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1490345568000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1490345568000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1490345568000},{"_id":"public/categories/index.html","hash":"193b4f05490c43d9609e22d53eb68f4d975dd5da","modified":1495350133360},{"_id":"public/tags/index.html","hash":"70d342b69fb8a711c14fb06521eaaef75c6dd2a8","modified":1495350133362},{"_id":"public/archives/index.html","hash":"a1c379a5557a7a24c5e4df2ca8e7f5135fc5d4b8","modified":1495350133363},{"_id":"public/archives/2016/index.html","hash":"7ef90d11522244f51dfb2864dcc8a51763cc1e22","modified":1495350133363},{"_id":"public/archives/2016/11/index.html","hash":"35bedf69cb819d70dbef5b1b957ade1b6e6284d0","modified":1495350133363},{"_id":"public/archives/2017/index.html","hash":"695740049d71ebd3398e2eba9c56b63f7f75c6c2","modified":1495350133363},{"_id":"public/archives/2017/02/index.html","hash":"0cfd883a7b263cd2863f7ead31ae8958d1db6ce4","modified":1495350133363},{"_id":"public/categories/deep-learning/index.html","hash":"ac06e50db87a269bda3f479d827febb493ac52bf","modified":1495350133363},{"_id":"public/tags/caffe/index.html","hash":"c0327ca0f96499e446926c0259e317056bcc1478","modified":1495350133363},{"_id":"public/tags/python/index.html","hash":"3570b53667709b1959650bc11673523a02953bca","modified":1495350133363},{"_id":"public/tags/YOLOv2/index.html","hash":"3423a197bd3b0e5b441c34b999e032af4fb057b0","modified":1495350133363},{"_id":"public/2016/11/11/2016-11-11-ubunt16.04下caffe环境安装/index.html","hash":"acc0f2a3e3650923e89c3142e4427e0226ca883e","modified":1495350133364},{"_id":"public/2017/02/07/2017-02-07-用YOLOv2训练自己的数据集/index.html","hash":"b141e90a779d1c4355ba272f5ef7e73a02ae5f39","modified":1495350133364},{"_id":"public/2017/02/08/2017-02-08-制作自己的图片数据集/index.html","hash":"d385fd9227c2a39ba78ca50c5d122c0fbc351861","modified":1495350133364},{"_id":"public/2017/02/21/2017-02-21-用faster-rcnn训练自己的数据集(python版)/index.html","hash":"00be07a71ea79f075b336154bd66867302d81d65","modified":1495350133364},{"_id":"public/index.html","hash":"6cd235e4154836f4d3acf1e2ac292cd20293e779","modified":1495350133364},{"_id":"public/images/avatar.jpg","hash":"e8b541d0375312a4b41917a0214ce15a2c17b100","modified":1495350133378},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1495350133378},{"_id":"public/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1495350133378},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1495350133378},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1495350133378},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1495350133378},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1495350133378},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1495350133378},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1495350133379},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1495350133379},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1495350133379},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1495350133379},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1495350133379},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1495350133379},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1495350133379},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1495350133379},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1495350133379},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1495350133379},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1495350133379},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1495350133379},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1495350133380},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1495350133380},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1495350133380},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1495350133380},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1495350133380},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1495350135293},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1495350135311},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1495350135331},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1495350135331},{"_id":"public/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1495350135331},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1495350135331},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1495350135332},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1495350135332},{"_id":"public/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1495350135332},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1495350135332},{"_id":"public/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1495350135332},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1495350135332},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1495350135332},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1495350135332},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1495350135332},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1495350135332},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1495350135332},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1495350135332},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1495350135333},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1495350135333},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1495350135333},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1495350135333},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1495350135333},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1495350135333},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1495350135333},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1495350135333},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1495350135333},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1495350135333},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1495350135333},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1495350135334},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1495350135334},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1495350135334},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1495350135334},{"_id":"public/css/main.css","hash":"47d6ea29e13d27dd2dcc0937b3e53b8a8aa6dcf9","modified":1495350135334},{"_id":"public/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1495350135334},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1495350135334},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1495350135334},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1495350135334},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1495350135334},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1495350135334},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1495350135334},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1495350135335},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1495350135335},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1495350135335},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1495350135335},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1495350135335},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1495350135335},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1495350135335},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1495350135336},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1495350135375}],"Category":[{"name":"deep learning","_id":"cj2ycuxi50004tdjcbmmct09y"}],"Data":[],"Page":[{"title":"categories","date":"2017-04-18T06:15:29.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-04-18 14:15:29\n---\n","updated":"2017-04-18T06:15:29.149Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cj2ycuxhe0001tdjc2km0dtnk","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2017-04-18T06:15:36.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-04-18 14:15:36\ntype: \"tags\"\n---\n","updated":"2017-04-18T06:21:54.462Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cj2ycuxi20003tdjc6uq8jr7a","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"post","title":"ubunt16.04下caffe环境安装","date":"2016-11-11T10:00:00.000Z","description":"ubunt16.04下caffe环境安装","_content":"\n\n## 一. 系统初始环境\n\n<!--more-->\n\n**系统:Ubuntu16.04**:  ubuntu-16.04-desktop-amd64.iso<br />\n\n**cuda安装文件**: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接[点击](https://developer.nvidia.com/cuda-downloads), linux-x86架构-ubuntu-16.04-deb(local)<br />\n**cudnn安装文件**: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接[点击](https://developer.nvidia.com/cudnn), 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大<br />\n**caffe源代码**: [github链接](https://github.com/BVLC/caffe) ,或者运行git clone https://github.com/BVLC/caffe.git<br />\n\n\n\n## 二. 安装cuda8.0\n1. sudo apt-get update\n2. sudo apt-get upgrade\n3. 删除自带显卡驱动 sudo apt-get --purge remove nvidia-*\n4. sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb  (这个文件包含了nvidia-367的驱动)\n5. 安装cuda:  sudo apt-get install cuda\n6. reboot\n\n**测试安装结果:**\n1. cd /usr/local/cuda/samples/1_Utilities/deviceQuery\n2. sudo make\n3.  ./deviceQuery\n\n**测试成功显示结果(部分):**\n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, <br />\n> CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 960M<br />\n> Result = PASS<br />\n\n如果出现Result = Failed之类的表示cuda安装失败\n\n**添加环境变量:**<br />\n在/etc/profile中添加<br />\nexport PATH=/usr/local/cuda-8.0/bin:$PATH<br />\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH<br />\n\n## 三. 安装cudnn5.0\n1. 解压安装包 <br />\ntar zxvf cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8<br />\n2. 复制文件<br />\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include/<br />\nsudo cp cuda/lib64/libcudnn.so.5.0.5 /usr/local/cuda/lib64/<br />\n3. 建立软链接<br />\ncd /usr/local/cuda/lib64/<br />\nsudo ln -s libcudnn.so.5.0.5 libcudnn.so.5<br />\nsudo ln -s libcudnn.so.5 libcudnn.so<br />\n\n## 四. 安装caffe\n\n1. 安装依赖库<br />\nsudo apt-get install build-essential<br />\nsudo apt-get install git cmake gedit doxygen<br />\nsudo apt-get install python-numpy python-pip cython easydict<br />\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler<br />\nsudo apt-get install libopencv-dev  (想使用opencv3的可以尝试跳过这一步自行安装opencv3)<br />\nsudo apt-get install --no-install-recommends libboost-all-dev<br />\nsudo apt-get install libatlas-base-dev<br />\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev<br />\n2. 下载源代码并解压<br />\ngit clone https://github.com/BVLC/caffe.git<br />\nunzip caffe-master.zip<br />\n3. 修改配置文件Make.config<br />\ncd caffe-master<br />\ncp Makefile.config.example Makefile.config<br />\nsudo gedit Makefile.config\n\n        开启GPU模式:将#USE_CUDNN := 1前的#注释去掉，表示使用cudnn，如果不是使用GPU，可以将#CPU_ONLY := 1前得注释去掉\n        pycaffe模块配置:修改\\#Whatever else you find you need goes here.\n        INCLUDE_DIRS:= $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\n        LIBRARY_DIRS:=$(PYTHON_LIB) /usr/local/lib /usr/lib/usr/lib/x86_64-linux-gnu/hdf5/serial\n \n如果要用opencv3编译的话还需要改动:\n\n\t\tLIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5\n\n\t\t# handle IO dependencies\n\t\tUSE_LEVELDB ?= 1\n\t\tUSE_LMDB ?= 1\n\t\tUSE_OPENCV ?= 1\n\n\t\tifeq ($(USE_LEVELDB), 1)\n\t\t\tLIBRARIES += leveldb snappy\n\t\tendif\n\t\tifeq ($(USE_LMDB), 1)\n\t\t\tLIBRARIES += lmdb\n\t\tendif\n\t\tifeq ($(USE_OPENCV), 1)\n\t\t\tLIBRARIES += opencv_core opencv_highgui opencv_imgproc \n\n\t\t\tifeq ($(OPENCV_VERSION), 3)\n\t\t\t\tLIBRARIES += opencv_imgcodecs\n\t\t\tendif\n\t\t\n\t\tendif\n4. 编译caffe<br />\nmkdir build <br />\ncd build <br />\ncmake .. <br />\nmake all -j8 <br />\n\n5. 编译pycaffe <br />\ncd caffe-master <br />\nmake pycaffe <br />\n\n6. 编译测试模块 <br />\nmake all <br />\nmake test <br />\nmake runtest <br />\n\n**测试成功显示结果(部分):** <br />\n>[----------] Global test environment tear-down <br />\n[==========] 2091 tests from 283 test cases ran. (415487 ms total) <br />\n[  PASSED  ] 2091 tests. <br />\n\n**添加环境变量:** <br />\n在~/.bashrc中添加 <br />\nexport PYTHONPATH=/home/hyzhan/caffe-master/caffe/python:$PYTHONPATH <br />\nexport PYTHONPATH=/home/hyzhan/caffe-master/:$PYTHONPATH <br />\n","source":"_posts/2016-11-11-ubunt16.04下caffe环境安装.md","raw":"---\nlayout: post\ntitle: \"ubunt16.04下caffe环境安装\"\ndate: 2016-11-11 18:00:00\ndescription: \"ubunt16.04下caffe环境安装\"\ncategory: [deep learning]\ntags: [caffe]\n---\n\n\n## 一. 系统初始环境\n\n<!--more-->\n\n**系统:Ubuntu16.04**:  ubuntu-16.04-desktop-amd64.iso<br />\n\n**cuda安装文件**: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接[点击](https://developer.nvidia.com/cuda-downloads), linux-x86架构-ubuntu-16.04-deb(local)<br />\n**cudnn安装文件**: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接[点击](https://developer.nvidia.com/cudnn), 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大<br />\n**caffe源代码**: [github链接](https://github.com/BVLC/caffe) ,或者运行git clone https://github.com/BVLC/caffe.git<br />\n\n\n\n## 二. 安装cuda8.0\n1. sudo apt-get update\n2. sudo apt-get upgrade\n3. 删除自带显卡驱动 sudo apt-get --purge remove nvidia-*\n4. sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb  (这个文件包含了nvidia-367的驱动)\n5. 安装cuda:  sudo apt-get install cuda\n6. reboot\n\n**测试安装结果:**\n1. cd /usr/local/cuda/samples/1_Utilities/deviceQuery\n2. sudo make\n3.  ./deviceQuery\n\n**测试成功显示结果(部分):**\n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, <br />\n> CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 960M<br />\n> Result = PASS<br />\n\n如果出现Result = Failed之类的表示cuda安装失败\n\n**添加环境变量:**<br />\n在/etc/profile中添加<br />\nexport PATH=/usr/local/cuda-8.0/bin:$PATH<br />\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH<br />\n\n## 三. 安装cudnn5.0\n1. 解压安装包 <br />\ntar zxvf cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8<br />\n2. 复制文件<br />\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include/<br />\nsudo cp cuda/lib64/libcudnn.so.5.0.5 /usr/local/cuda/lib64/<br />\n3. 建立软链接<br />\ncd /usr/local/cuda/lib64/<br />\nsudo ln -s libcudnn.so.5.0.5 libcudnn.so.5<br />\nsudo ln -s libcudnn.so.5 libcudnn.so<br />\n\n## 四. 安装caffe\n\n1. 安装依赖库<br />\nsudo apt-get install build-essential<br />\nsudo apt-get install git cmake gedit doxygen<br />\nsudo apt-get install python-numpy python-pip cython easydict<br />\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler<br />\nsudo apt-get install libopencv-dev  (想使用opencv3的可以尝试跳过这一步自行安装opencv3)<br />\nsudo apt-get install --no-install-recommends libboost-all-dev<br />\nsudo apt-get install libatlas-base-dev<br />\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev<br />\n2. 下载源代码并解压<br />\ngit clone https://github.com/BVLC/caffe.git<br />\nunzip caffe-master.zip<br />\n3. 修改配置文件Make.config<br />\ncd caffe-master<br />\ncp Makefile.config.example Makefile.config<br />\nsudo gedit Makefile.config\n\n        开启GPU模式:将#USE_CUDNN := 1前的#注释去掉，表示使用cudnn，如果不是使用GPU，可以将#CPU_ONLY := 1前得注释去掉\n        pycaffe模块配置:修改\\#Whatever else you find you need goes here.\n        INCLUDE_DIRS:= $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\n        LIBRARY_DIRS:=$(PYTHON_LIB) /usr/local/lib /usr/lib/usr/lib/x86_64-linux-gnu/hdf5/serial\n \n如果要用opencv3编译的话还需要改动:\n\n\t\tLIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5\n\n\t\t# handle IO dependencies\n\t\tUSE_LEVELDB ?= 1\n\t\tUSE_LMDB ?= 1\n\t\tUSE_OPENCV ?= 1\n\n\t\tifeq ($(USE_LEVELDB), 1)\n\t\t\tLIBRARIES += leveldb snappy\n\t\tendif\n\t\tifeq ($(USE_LMDB), 1)\n\t\t\tLIBRARIES += lmdb\n\t\tendif\n\t\tifeq ($(USE_OPENCV), 1)\n\t\t\tLIBRARIES += opencv_core opencv_highgui opencv_imgproc \n\n\t\t\tifeq ($(OPENCV_VERSION), 3)\n\t\t\t\tLIBRARIES += opencv_imgcodecs\n\t\t\tendif\n\t\t\n\t\tendif\n4. 编译caffe<br />\nmkdir build <br />\ncd build <br />\ncmake .. <br />\nmake all -j8 <br />\n\n5. 编译pycaffe <br />\ncd caffe-master <br />\nmake pycaffe <br />\n\n6. 编译测试模块 <br />\nmake all <br />\nmake test <br />\nmake runtest <br />\n\n**测试成功显示结果(部分):** <br />\n>[----------] Global test environment tear-down <br />\n[==========] 2091 tests from 283 test cases ran. (415487 ms total) <br />\n[  PASSED  ] 2091 tests. <br />\n\n**添加环境变量:** <br />\n在~/.bashrc中添加 <br />\nexport PYTHONPATH=/home/hyzhan/caffe-master/caffe/python:$PYTHONPATH <br />\nexport PYTHONPATH=/home/hyzhan/caffe-master/:$PYTHONPATH <br />\n","slug":"2016-11-11-ubunt16.04下caffe环境安装","published":1,"updated":"2017-04-18T05:45:39.187Z","comments":1,"photos":[],"link":"","_id":"cj2ycuxgx0000tdjcdz8922ze","content":"<h2 id=\"一-系统初始环境\"><a href=\"#一-系统初始环境\" class=\"headerlink\" title=\"一. 系统初始环境\"></a>一. 系统初始环境</h2><a id=\"more\"></a>\n<p><strong>系统:Ubuntu16.04</strong>:  ubuntu-16.04-desktop-amd64.iso<br></p>\n<p><strong>cuda安装文件</strong>: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接<a href=\"https://developer.nvidia.com/cuda-downloads\" target=\"_blank\" rel=\"external\">点击</a>, linux-x86架构-ubuntu-16.04-deb(local)<br><br><strong>cudnn安装文件</strong>: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接<a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"external\">点击</a>, 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大<br><br><strong>caffe源代码</strong>: <a href=\"https://github.com/BVLC/caffe\" target=\"_blank\" rel=\"external\">github链接</a> ,或者运行git clone <a href=\"https://github.com/BVLC/caffe.git\" target=\"_blank\" rel=\"external\">https://github.com/BVLC/caffe.git</a><br></p>\n<h2 id=\"二-安装cuda8-0\"><a href=\"#二-安装cuda8-0\" class=\"headerlink\" title=\"二. 安装cuda8.0\"></a>二. 安装cuda8.0</h2><ol>\n<li>sudo apt-get update</li>\n<li>sudo apt-get upgrade</li>\n<li>删除自带显卡驱动 sudo apt-get –purge remove nvidia-*</li>\n<li>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb  (这个文件包含了nvidia-367的驱动)</li>\n<li>安装cuda:  sudo apt-get install cuda</li>\n<li>reboot</li>\n</ol>\n<p><strong>测试安装结果:</strong></p>\n<ol>\n<li>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</li>\n<li>sudo make</li>\n<li>./deviceQuery</li>\n</ol>\n<p><strong>测试成功显示结果(部分):</strong></p>\n<blockquote>\n<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, <br><br>CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 960M<br><br>Result = PASS<br></p>\n</blockquote>\n<p>如果出现Result = Failed之类的表示cuda安装失败</p>\n<p><strong>添加环境变量:</strong><br><br>在/etc/profile中添加<br><br>export PATH=/usr/local/cuda-8.0/bin:$PATH<br><br>export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH<br></p>\n<h2 id=\"三-安装cudnn5-0\"><a href=\"#三-安装cudnn5-0\" class=\"headerlink\" title=\"三. 安装cudnn5.0\"></a>三. 安装cudnn5.0</h2><ol>\n<li>解压安装包 <br><br>tar zxvf cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8<br></li>\n<li>复制文件<br><br>sudo cp cuda/include/cudnn.h /usr/local/cuda/include/<br><br>sudo cp cuda/lib64/libcudnn.so.5.0.5 /usr/local/cuda/lib64/<br></li>\n<li>建立软链接<br><br>cd /usr/local/cuda/lib64/<br><br>sudo ln -s libcudnn.so.5.0.5 libcudnn.so.5<br><br>sudo ln -s libcudnn.so.5 libcudnn.so<br></li>\n</ol>\n<h2 id=\"四-安装caffe\"><a href=\"#四-安装caffe\" class=\"headerlink\" title=\"四. 安装caffe\"></a>四. 安装caffe</h2><ol>\n<li>安装依赖库<br><br>sudo apt-get install build-essential<br><br>sudo apt-get install git cmake gedit doxygen<br><br>sudo apt-get install python-numpy python-pip cython easydict<br><br>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler<br><br>sudo apt-get install libopencv-dev  (想使用opencv3的可以尝试跳过这一步自行安装opencv3)<br><br>sudo apt-get install –no-install-recommends libboost-all-dev<br><br>sudo apt-get install libatlas-base-dev<br><br>sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev<br></li>\n<li>下载源代码并解压<br><br>git clone <a href=\"https://github.com/BVLC/caffe.git\" target=\"_blank\" rel=\"external\">https://github.com/BVLC/caffe.git</a><br><br>unzip caffe-master.zip<br></li>\n<li><p>修改配置文件Make.config<br><br>cd caffe-master<br><br>cp Makefile.config.example Makefile.config<br><br>sudo gedit Makefile.config</p>\n<pre><code>开启GPU模式:将#USE_CUDNN := 1前的#注释去掉，表示使用cudnn，如果不是使用GPU，可以将#CPU_ONLY := 1前得注释去掉\npycaffe模块配置:修改\\#Whatever else you find you need goes here.\nINCLUDE_DIRS:= $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\nLIBRARY_DIRS:=$(PYTHON_LIB) /usr/local/lib /usr/lib/usr/lib/x86_64-linux-gnu/hdf5/serial\n</code></pre></li>\n</ol>\n<p>如果要用opencv3编译的话还需要改动:</p>\n<pre><code>LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5\n\n# handle IO dependencies\nUSE_LEVELDB ?= 1\nUSE_LMDB ?= 1\nUSE_OPENCV ?= 1\n\nifeq ($(USE_LEVELDB), 1)\n    LIBRARIES += leveldb snappy\nendif\nifeq ($(USE_LMDB), 1)\n    LIBRARIES += lmdb\nendif\nifeq ($(USE_OPENCV), 1)\n    LIBRARIES += opencv_core opencv_highgui opencv_imgproc \n\n    ifeq ($(OPENCV_VERSION), 3)\n        LIBRARIES += opencv_imgcodecs\n    endif\n\nendif\n</code></pre><ol>\n<li><p>编译caffe<br><br>mkdir build <br><br>cd build <br><br>cmake .. <br><br>make all -j8 <br></p>\n</li>\n<li><p>编译pycaffe <br><br>cd caffe-master <br><br>make pycaffe <br></p>\n</li>\n<li><p>编译测试模块 <br><br>make all <br><br>make test <br><br>make runtest <br></p>\n</li>\n</ol>\n<p><strong>测试成功显示结果(部分):</strong> <br></p>\n<blockquote>\n<p>[———-] Global test environment tear-down <br><br>[==========] 2091 tests from 283 test cases ran. (415487 ms total) <br><br>[  PASSED  ] 2091 tests. <br></p>\n</blockquote>\n<p><strong>添加环境变量:</strong> <br><br>在~/.bashrc中添加 <br><br>export PYTHONPATH=/home/hyzhan/caffe-master/caffe/python:$PYTHONPATH <br><br>export PYTHONPATH=/home/hyzhan/caffe-master/:$PYTHONPATH <br></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"一-系统初始环境\"><a href=\"#一-系统初始环境\" class=\"headerlink\" title=\"一. 系统初始环境\"></a>一. 系统初始环境</h2>","more":"<p><strong>系统:Ubuntu16.04</strong>:  ubuntu-16.04-desktop-amd64.iso<br /></p>\n<p><strong>cuda安装文件</strong>: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接<a href=\"https://developer.nvidia.com/cuda-downloads\">点击</a>, linux-x86架构-ubuntu-16.04-deb(local)<br /><br><strong>cudnn安装文件</strong>: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接<a href=\"https://developer.nvidia.com/cudnn\">点击</a>, 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大<br /><br><strong>caffe源代码</strong>: <a href=\"https://github.com/BVLC/caffe\">github链接</a> ,或者运行git clone <a href=\"https://github.com/BVLC/caffe.git\">https://github.com/BVLC/caffe.git</a><br /></p>\n<h2 id=\"二-安装cuda8-0\"><a href=\"#二-安装cuda8-0\" class=\"headerlink\" title=\"二. 安装cuda8.0\"></a>二. 安装cuda8.0</h2><ol>\n<li>sudo apt-get update</li>\n<li>sudo apt-get upgrade</li>\n<li>删除自带显卡驱动 sudo apt-get –purge remove nvidia-*</li>\n<li>sudo dpkg -i cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb  (这个文件包含了nvidia-367的驱动)</li>\n<li>安装cuda:  sudo apt-get install cuda</li>\n<li>reboot</li>\n</ol>\n<p><strong>测试安装结果:</strong></p>\n<ol>\n<li>cd /usr/local/cuda/samples/1_Utilities/deviceQuery</li>\n<li>sudo make</li>\n<li>./deviceQuery</li>\n</ol>\n<p><strong>测试成功显示结果(部分):</strong></p>\n<blockquote>\n<p>deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, <br /><br>CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GTX 960M<br /><br>Result = PASS<br /></p>\n</blockquote>\n<p>如果出现Result = Failed之类的表示cuda安装失败</p>\n<p><strong>添加环境变量:</strong><br /><br>在/etc/profile中添加<br /><br>export PATH=/usr/local/cuda-8.0/bin:$PATH<br /><br>export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH<br /></p>\n<h2 id=\"三-安装cudnn5-0\"><a href=\"#三-安装cudnn5-0\" class=\"headerlink\" title=\"三. 安装cudnn5.0\"></a>三. 安装cudnn5.0</h2><ol>\n<li>解压安装包 <br /><br>tar zxvf cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8<br /></li>\n<li>复制文件<br /><br>sudo cp cuda/include/cudnn.h /usr/local/cuda/include/<br /><br>sudo cp cuda/lib64/libcudnn.so.5.0.5 /usr/local/cuda/lib64/<br /></li>\n<li>建立软链接<br /><br>cd /usr/local/cuda/lib64/<br /><br>sudo ln -s libcudnn.so.5.0.5 libcudnn.so.5<br /><br>sudo ln -s libcudnn.so.5 libcudnn.so<br /></li>\n</ol>\n<h2 id=\"四-安装caffe\"><a href=\"#四-安装caffe\" class=\"headerlink\" title=\"四. 安装caffe\"></a>四. 安装caffe</h2><ol>\n<li>安装依赖库<br /><br>sudo apt-get install build-essential<br /><br>sudo apt-get install git cmake gedit doxygen<br /><br>sudo apt-get install python-numpy python-pip cython easydict<br /><br>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libhdf5-serial-dev protobuf-compiler<br /><br>sudo apt-get install libopencv-dev  (想使用opencv3的可以尝试跳过这一步自行安装opencv3)<br /><br>sudo apt-get install –no-install-recommends libboost-all-dev<br /><br>sudo apt-get install libatlas-base-dev<br /><br>sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev<br /></li>\n<li>下载源代码并解压<br /><br>git clone <a href=\"https://github.com/BVLC/caffe.git\">https://github.com/BVLC/caffe.git</a><br /><br>unzip caffe-master.zip<br /></li>\n<li><p>修改配置文件Make.config<br /><br>cd caffe-master<br /><br>cp Makefile.config.example Makefile.config<br /><br>sudo gedit Makefile.config</p>\n<pre><code>开启GPU模式:将#USE_CUDNN := 1前的#注释去掉，表示使用cudnn，如果不是使用GPU，可以将#CPU_ONLY := 1前得注释去掉\npycaffe模块配置:修改\\#Whatever else you find you need goes here.\nINCLUDE_DIRS:= $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\nLIBRARY_DIRS:=$(PYTHON_LIB) /usr/local/lib /usr/lib/usr/lib/x86_64-linux-gnu/hdf5/serial\n</code></pre></li>\n</ol>\n<p>如果要用opencv3编译的话还需要改动:</p>\n<pre><code>LIBRARIES += glog gflags protobuf boost_system boost_filesystem m hdf5_hl hdf5\n\n# handle IO dependencies\nUSE_LEVELDB ?= 1\nUSE_LMDB ?= 1\nUSE_OPENCV ?= 1\n\nifeq ($(USE_LEVELDB), 1)\n    LIBRARIES += leveldb snappy\nendif\nifeq ($(USE_LMDB), 1)\n    LIBRARIES += lmdb\nendif\nifeq ($(USE_OPENCV), 1)\n    LIBRARIES += opencv_core opencv_highgui opencv_imgproc \n\n    ifeq ($(OPENCV_VERSION), 3)\n        LIBRARIES += opencv_imgcodecs\n    endif\n\nendif\n</code></pre><ol>\n<li><p>编译caffe<br /><br>mkdir build <br /><br>cd build <br /><br>cmake .. <br /><br>make all -j8 <br /></p>\n</li>\n<li><p>编译pycaffe <br /><br>cd caffe-master <br /><br>make pycaffe <br /></p>\n</li>\n<li><p>编译测试模块 <br /><br>make all <br /><br>make test <br /><br>make runtest <br /></p>\n</li>\n</ol>\n<p><strong>测试成功显示结果(部分):</strong> <br /></p>\n<blockquote>\n<p>[———-] Global test environment tear-down <br /><br>[==========] 2091 tests from 283 test cases ran. (415487 ms total) <br /><br>[  PASSED  ] 2091 tests. <br /></p>\n</blockquote>\n<p><strong>添加环境变量:</strong> <br /><br>在~/.bashrc中添加 <br /><br>export PYTHONPATH=/home/hyzhan/caffe-master/caffe/python:$PYTHONPATH <br /><br>export PYTHONPATH=/home/hyzhan/caffe-master/:$PYTHONPATH <br /></p>"},{"layout":"post","title":"制作自己的图片数据集(VOC2007格式)","date":"2017-02-08T04:04:00.000Z","description":"制作自己的图片数据集,用于训练需要的模型","_content":"\n制作自己的图片数据集(VOC2007格式),用于训练需要的模型,用于faster-rcnn,YOLO等\n\n<!--more-->\n\n## 一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)\n<br>\n\n## 二. 标注图片数据(Label Image Data)\n\nrename_images.py create_trainval.py delete_file_firstRow.py等文件在[make_own_dataset](https://github.com/hyzhan/make_own_dataset#labelimg)\n\n或者直接git clone https://github.com/hyzhan/make_own_dataset.git\n\n非常感谢tzutalin提供的标注工具 [github](https://github.com/tzutalin/labelImg)\n\nThanks to tzutalin.\n\n# LabelImg\n\n[![Build Status](https://travis-ci.org/tzutalin/labelImg.png)](https://travis-ci.org/tzutalin/labelImg)\n\nLabelImg is a graphical image annotation tool.\n\nIt is written in Python and uses Qt for its graphical interface.\n\nThe annotation file will be saved as an XML file. The annotation format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\n## Dependencies\n* Linux/Ubuntu/Mac\n\nRequires at least [Python 2.6](http://www.python.org/getit/) and has been tested with [PyQt\n4.8](http://www.riverbankcomputing.co.uk/software/pyqt/intro).\n\nIn order to build the resource and assets, you need to install pyqt4-dev-tools and lxml:\n```\n$ sudo apt-get install pyqt4-dev-tools\n$ sudo pip install lxml\n$ make all\n$ ./labelImg.py\n```\n\nMac requires \"$ brew install libxml2\" when installing lxml\n\n* Windows\n\nNeed to download and setup [Python 2.6](https://www.python.org/downloads/windows/) or later and [PyQt4](https://www.riverbankcomputing.com/software/pyqt/download). Also, you need to install other python dependencies.\n\nOpen cmd and go to [labelImg]\n\n```\n$ pyrcc4 -o resources.py resources.qrc\n$ python labelImg.py\n```\n\n## Usage\nAfter cloning the code, you should run `$ make all` to generate the resource file.\n\nYou can then start annotating by running `$ ./labelImg.py`. For usage\ninstructions you can see [Here](https://youtu.be/p0nR2YsCY_U)\n\nAt the moment annotations are saved as an XML file. The format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\nYou can also see [ImageNet Utils](https://github.com/tzutalin/ImageNet_Utils) to download image, create a label text for machine learning, etc\n\n\n### General steps from scratch\n\n* Build and launch: `$ make all; python labelImg.py`\n\n* Click 'Change default saved annotation folder' in Menu/File\n\n* Click 'Open Dir'\n\n* Click 'Create RectBox'\n\nThe annotation will be saved to the folder you specify\n\n### Create pre-defined classes\n\nYou can edit the [data/predefined_classes.txt](https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt) to load pre-defined classes\n\n### Hotkeys\n\n* Ctrl + r : Change the defult target dir which saving annotation files\n\n* Ctrl + n : Create a bounding box\n\n* Ctrl + s : Save\n\n* n : Next image\n\n* p : Previous image\n\n### How to contribute\nSend a pull request\n\n### License\n[License](LICENSE.md)\n\n\n(1).安装依赖库\n```\n$ sudo apt-get install pyqt4-dev-tools\n$ sudo pip install lxml\n$ make all\n```\n(2).图片名称批量修改\n    将图片名称统一后方便后期工作,执行:\n    ```\n    python rename_images.py\n    ```\n默认图片存放路径是在JPEGImages下,执行成功后会在该文件夹下生成tmp文件夹,里面有重命名后\n的图片文件,备份或删除原图片,在JPEGImages下仅保留重命名后的图片文件\n\n(3). 修改标签文件\n\n修改data文件下的predefined_classes.txt文件,改成自己所需要分类的类别名称,限英文\n\n(4).执行标注程序\n```\n./labelImg.py\n```\nPS.快捷键\n* Ctrl + r : Change the defult target dir which saving annotation files\n\n* Ctrl + n : Create a bounding box\n\n* Ctrl + s : Save\n\n* n : Next image\n\n* p : Previous image\n\n建议用opendir打开图片所在文件夹后再按Ctrl + r选择保存xml文件的位置(建议放在xml文件夹下),\n以免与图片混合起来,方便后期工作.\n\n(5). 格式化xml文件(可选)\n\n部分机器会在生成的xml文件加上版本号,后期训练时需要将生成的xml文件的首行<?xml version=\"1.0\" ?>去除,执行:\n    ```\n    python delete_file_firstRow.py\n    ```\n执行成功后会在该文件夹下生成Annotations文件夹,里面有格式化后的xml文件\n\n(6). 分割数据集\n\n训练时需要有训练数据集,测试数据集等txt文件,执行:\n    ```\n    python create_trainval.py\n    ```\n默认图片存放路径是在JPEGImages下,执行成功后会在生成ImageSets/Main/文件夹,里面有\n四个txt文件test.txt,train.txt,trainval.txt,val.txt\n\n(7).汇总数据集\n\n将得到的Annotations,ImageSets,JPEGImages文件夹放在VOC2007文件夹(没有则新建一个)即可\n","source":"_posts/2017-02-08-制作自己的图片数据集.md","raw":"---\nlayout: post\ntitle: \"制作自己的图片数据集(VOC2007格式)\"\ndate: 2017-02-08 12:04:00\ndescription: \"制作自己的图片数据集,用于训练需要的模型\"\ncategory: [deep learning]\ntags: [python]\n---\n\n制作自己的图片数据集(VOC2007格式),用于训练需要的模型,用于faster-rcnn,YOLO等\n\n<!--more-->\n\n## 一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)\n<br>\n\n## 二. 标注图片数据(Label Image Data)\n\nrename_images.py create_trainval.py delete_file_firstRow.py等文件在[make_own_dataset](https://github.com/hyzhan/make_own_dataset#labelimg)\n\n或者直接git clone https://github.com/hyzhan/make_own_dataset.git\n\n非常感谢tzutalin提供的标注工具 [github](https://github.com/tzutalin/labelImg)\n\nThanks to tzutalin.\n\n# LabelImg\n\n[![Build Status](https://travis-ci.org/tzutalin/labelImg.png)](https://travis-ci.org/tzutalin/labelImg)\n\nLabelImg is a graphical image annotation tool.\n\nIt is written in Python and uses Qt for its graphical interface.\n\nThe annotation file will be saved as an XML file. The annotation format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\n## Dependencies\n* Linux/Ubuntu/Mac\n\nRequires at least [Python 2.6](http://www.python.org/getit/) and has been tested with [PyQt\n4.8](http://www.riverbankcomputing.co.uk/software/pyqt/intro).\n\nIn order to build the resource and assets, you need to install pyqt4-dev-tools and lxml:\n```\n$ sudo apt-get install pyqt4-dev-tools\n$ sudo pip install lxml\n$ make all\n$ ./labelImg.py\n```\n\nMac requires \"$ brew install libxml2\" when installing lxml\n\n* Windows\n\nNeed to download and setup [Python 2.6](https://www.python.org/downloads/windows/) or later and [PyQt4](https://www.riverbankcomputing.com/software/pyqt/download). Also, you need to install other python dependencies.\n\nOpen cmd and go to [labelImg]\n\n```\n$ pyrcc4 -o resources.py resources.qrc\n$ python labelImg.py\n```\n\n## Usage\nAfter cloning the code, you should run `$ make all` to generate the resource file.\n\nYou can then start annotating by running `$ ./labelImg.py`. For usage\ninstructions you can see [Here](https://youtu.be/p0nR2YsCY_U)\n\nAt the moment annotations are saved as an XML file. The format is PASCAL VOC format, and the format is the same as [ImageNet](http://www.image-net.org/)\n\nYou can also see [ImageNet Utils](https://github.com/tzutalin/ImageNet_Utils) to download image, create a label text for machine learning, etc\n\n\n### General steps from scratch\n\n* Build and launch: `$ make all; python labelImg.py`\n\n* Click 'Change default saved annotation folder' in Menu/File\n\n* Click 'Open Dir'\n\n* Click 'Create RectBox'\n\nThe annotation will be saved to the folder you specify\n\n### Create pre-defined classes\n\nYou can edit the [data/predefined_classes.txt](https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt) to load pre-defined classes\n\n### Hotkeys\n\n* Ctrl + r : Change the defult target dir which saving annotation files\n\n* Ctrl + n : Create a bounding box\n\n* Ctrl + s : Save\n\n* n : Next image\n\n* p : Previous image\n\n### How to contribute\nSend a pull request\n\n### License\n[License](LICENSE.md)\n\n\n(1).安装依赖库\n```\n$ sudo apt-get install pyqt4-dev-tools\n$ sudo pip install lxml\n$ make all\n```\n(2).图片名称批量修改\n    将图片名称统一后方便后期工作,执行:\n    ```\n    python rename_images.py\n    ```\n默认图片存放路径是在JPEGImages下,执行成功后会在该文件夹下生成tmp文件夹,里面有重命名后\n的图片文件,备份或删除原图片,在JPEGImages下仅保留重命名后的图片文件\n\n(3). 修改标签文件\n\n修改data文件下的predefined_classes.txt文件,改成自己所需要分类的类别名称,限英文\n\n(4).执行标注程序\n```\n./labelImg.py\n```\nPS.快捷键\n* Ctrl + r : Change the defult target dir which saving annotation files\n\n* Ctrl + n : Create a bounding box\n\n* Ctrl + s : Save\n\n* n : Next image\n\n* p : Previous image\n\n建议用opendir打开图片所在文件夹后再按Ctrl + r选择保存xml文件的位置(建议放在xml文件夹下),\n以免与图片混合起来,方便后期工作.\n\n(5). 格式化xml文件(可选)\n\n部分机器会在生成的xml文件加上版本号,后期训练时需要将生成的xml文件的首行<?xml version=\"1.0\" ?>去除,执行:\n    ```\n    python delete_file_firstRow.py\n    ```\n执行成功后会在该文件夹下生成Annotations文件夹,里面有格式化后的xml文件\n\n(6). 分割数据集\n\n训练时需要有训练数据集,测试数据集等txt文件,执行:\n    ```\n    python create_trainval.py\n    ```\n默认图片存放路径是在JPEGImages下,执行成功后会在生成ImageSets/Main/文件夹,里面有\n四个txt文件test.txt,train.txt,trainval.txt,val.txt\n\n(7).汇总数据集\n\n将得到的Annotations,ImageSets,JPEGImages文件夹放在VOC2007文件夹(没有则新建一个)即可\n","slug":"2017-02-08-制作自己的图片数据集","published":1,"updated":"2017-04-18T05:45:39.191Z","comments":1,"photos":[],"link":"","_id":"cj2ycuxhx0002tdjcb9ljqff1","content":"<p>制作自己的图片数据集(VOC2007格式),用于训练需要的模型,用于faster-rcnn,YOLO等</p>\n<a id=\"more\"></a>\n<h2 id=\"一-获取数据-自行拍照或爬虫下载-不详述-Get-data-telephone-or-spam-No-more-details\"><a href=\"#一-获取数据-自行拍照或爬虫下载-不详述-Get-data-telephone-or-spam-No-more-details\" class=\"headerlink\" title=\"一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)\"></a>一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)</h2><p><br></p>\n<h2 id=\"二-标注图片数据-Label-Image-Data\"><a href=\"#二-标注图片数据-Label-Image-Data\" class=\"headerlink\" title=\"二. 标注图片数据(Label Image Data)\"></a>二. 标注图片数据(Label Image Data)</h2><p>rename_images.py create_trainval.py delete_file_firstRow.py等文件在<a href=\"https://github.com/hyzhan/make_own_dataset#labelimg\" target=\"_blank\" rel=\"external\">make_own_dataset</a></p>\n<p>或者直接git clone <a href=\"https://github.com/hyzhan/make_own_dataset.git\" target=\"_blank\" rel=\"external\">https://github.com/hyzhan/make_own_dataset.git</a></p>\n<p>非常感谢tzutalin提供的标注工具 <a href=\"https://github.com/tzutalin/labelImg\" target=\"_blank\" rel=\"external\">github</a></p>\n<p>Thanks to tzutalin.</p>\n<h1 id=\"LabelImg\"><a href=\"#LabelImg\" class=\"headerlink\" title=\"LabelImg\"></a>LabelImg</h1><p><a href=\"https://travis-ci.org/tzutalin/labelImg\" target=\"_blank\" rel=\"external\"><img src=\"https://travis-ci.org/tzutalin/labelImg.png\" alt=\"Build Status\"></a></p>\n<p>LabelImg is a graphical image annotation tool.</p>\n<p>It is written in Python and uses Qt for its graphical interface.</p>\n<p>The annotation file will be saved as an XML file. The annotation format is PASCAL VOC format, and the format is the same as <a href=\"http://www.image-net.org/\" target=\"_blank\" rel=\"external\">ImageNet</a></p>\n<h2 id=\"Dependencies\"><a href=\"#Dependencies\" class=\"headerlink\" title=\"Dependencies\"></a>Dependencies</h2><ul>\n<li>Linux/Ubuntu/Mac</li>\n</ul>\n<p>Requires at least <a href=\"http://www.python.org/getit/\" target=\"_blank\" rel=\"external\">Python 2.6</a> and has been tested with <a href=\"http://www.riverbankcomputing.co.uk/software/pyqt/intro\" target=\"_blank\" rel=\"external\">PyQt<br>4.8</a>.</p>\n<p>In order to build the resource and assets, you need to install pyqt4-dev-tools and lxml:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo apt-get install pyqt4-dev-tools</div><div class=\"line\">$ sudo pip install lxml</div><div class=\"line\">$ make all</div><div class=\"line\">$ ./labelImg.py</div></pre></td></tr></table></figure></p>\n<p>Mac requires “$ brew install libxml2” when installing lxml</p>\n<ul>\n<li>Windows</li>\n</ul>\n<p>Need to download and setup <a href=\"https://www.python.org/downloads/windows/\" target=\"_blank\" rel=\"external\">Python 2.6</a> or later and <a href=\"https://www.riverbankcomputing.com/software/pyqt/download\" target=\"_blank\" rel=\"external\">PyQt4</a>. Also, you need to install other python dependencies.</p>\n<p>Open cmd and go to [labelImg]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ pyrcc4 -o resources.py resources.qrc</div><div class=\"line\">$ python labelImg.py</div></pre></td></tr></table></figure>\n<h2 id=\"Usage\"><a href=\"#Usage\" class=\"headerlink\" title=\"Usage\"></a>Usage</h2><p>After cloning the code, you should run <code>$ make all</code> to generate the resource file.</p>\n<p>You can then start annotating by running <code>$ ./labelImg.py</code>. For usage<br>instructions you can see <a href=\"https://youtu.be/p0nR2YsCY_U\" target=\"_blank\" rel=\"external\">Here</a></p>\n<p>At the moment annotations are saved as an XML file. The format is PASCAL VOC format, and the format is the same as <a href=\"http://www.image-net.org/\" target=\"_blank\" rel=\"external\">ImageNet</a></p>\n<p>You can also see <a href=\"https://github.com/tzutalin/ImageNet_Utils\" target=\"_blank\" rel=\"external\">ImageNet Utils</a> to download image, create a label text for machine learning, etc</p>\n<h3 id=\"General-steps-from-scratch\"><a href=\"#General-steps-from-scratch\" class=\"headerlink\" title=\"General steps from scratch\"></a>General steps from scratch</h3><ul>\n<li><p>Build and launch: <code>$ make all; python labelImg.py</code></p>\n</li>\n<li><p>Click ‘Change default saved annotation folder’ in Menu/File</p>\n</li>\n<li><p>Click ‘Open Dir’</p>\n</li>\n<li><p>Click ‘Create RectBox’</p>\n</li>\n</ul>\n<p>The annotation will be saved to the folder you specify</p>\n<h3 id=\"Create-pre-defined-classes\"><a href=\"#Create-pre-defined-classes\" class=\"headerlink\" title=\"Create pre-defined classes\"></a>Create pre-defined classes</h3><p>You can edit the <a href=\"https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt\" target=\"_blank\" rel=\"external\">data/predefined_classes.txt</a> to load pre-defined classes</p>\n<h3 id=\"Hotkeys\"><a href=\"#Hotkeys\" class=\"headerlink\" title=\"Hotkeys\"></a>Hotkeys</h3><ul>\n<li><p>Ctrl + r : Change the defult target dir which saving annotation files</p>\n</li>\n<li><p>Ctrl + n : Create a bounding box</p>\n</li>\n<li><p>Ctrl + s : Save</p>\n</li>\n<li><p>n : Next image</p>\n</li>\n<li><p>p : Previous image</p>\n</li>\n</ul>\n<h3 id=\"How-to-contribute\"><a href=\"#How-to-contribute\" class=\"headerlink\" title=\"How to contribute\"></a>How to contribute</h3><p>Send a pull request</p>\n<h3 id=\"License\"><a href=\"#License\" class=\"headerlink\" title=\"License\"></a>License</h3><p><a href=\"LICENSE.md\">License</a></p>\n<p>(1).安装依赖库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo apt-get install pyqt4-dev-tools</div><div class=\"line\">$ sudo pip install lxml</div><div class=\"line\">$ make all</div></pre></td></tr></table></figure></p>\n<p>(2).图片名称批量修改<br>    将图片名称统一后方便后期工作,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python rename_images.py</div></pre></td></tr></table></figure></p>\n<p>默认图片存放路径是在JPEGImages下,执行成功后会在该文件夹下生成tmp文件夹,里面有重命名后<br>的图片文件,备份或删除原图片,在JPEGImages下仅保留重命名后的图片文件</p>\n<p>(3). 修改标签文件</p>\n<p>修改data文件下的predefined_classes.txt文件,改成自己所需要分类的类别名称,限英文</p>\n<p>(4).执行标注程序<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./labelImg.py</div></pre></td></tr></table></figure></p>\n<p>PS.快捷键</p>\n<ul>\n<li><p>Ctrl + r : Change the defult target dir which saving annotation files</p>\n</li>\n<li><p>Ctrl + n : Create a bounding box</p>\n</li>\n<li><p>Ctrl + s : Save</p>\n</li>\n<li><p>n : Next image</p>\n</li>\n<li><p>p : Previous image</p>\n</li>\n</ul>\n<p>建议用opendir打开图片所在文件夹后再按Ctrl + r选择保存xml文件的位置(建议放在xml文件夹下),<br>以免与图片混合起来,方便后期工作.</p>\n<p>(5). 格式化xml文件(可选)</p>\n<p>部分机器会在生成的xml文件加上版本号,后期训练时需要将生成的xml文件的首行&lt;?xml version=”1.0” ?&gt;去除,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python delete_file_firstRow.py</div></pre></td></tr></table></figure></p>\n<p>执行成功后会在该文件夹下生成Annotations文件夹,里面有格式化后的xml文件</p>\n<p>(6). 分割数据集</p>\n<p>训练时需要有训练数据集,测试数据集等txt文件,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python create_trainval.py</div></pre></td></tr></table></figure></p>\n<p>默认图片存放路径是在JPEGImages下,执行成功后会在生成ImageSets/Main/文件夹,里面有<br>四个txt文件test.txt,train.txt,trainval.txt,val.txt</p>\n<p>(7).汇总数据集</p>\n<p>将得到的Annotations,ImageSets,JPEGImages文件夹放在VOC2007文件夹(没有则新建一个)即可</p>\n","site":{"data":{}},"excerpt":"<p>制作自己的图片数据集(VOC2007格式),用于训练需要的模型,用于faster-rcnn,YOLO等</p>","more":"<h2 id=\"一-获取数据-自行拍照或爬虫下载-不详述-Get-data-telephone-or-spam-No-more-details\"><a href=\"#一-获取数据-自行拍照或爬虫下载-不详述-Get-data-telephone-or-spam-No-more-details\" class=\"headerlink\" title=\"一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)\"></a>一. 获取数据(自行拍照或爬虫下载,不详述)Get data(telephone or spam,No more details)</h2><p><br></p>\n<h2 id=\"二-标注图片数据-Label-Image-Data\"><a href=\"#二-标注图片数据-Label-Image-Data\" class=\"headerlink\" title=\"二. 标注图片数据(Label Image Data)\"></a>二. 标注图片数据(Label Image Data)</h2><p>rename_images.py create_trainval.py delete_file_firstRow.py等文件在<a href=\"https://github.com/hyzhan/make_own_dataset#labelimg\">make_own_dataset</a></p>\n<p>或者直接git clone <a href=\"https://github.com/hyzhan/make_own_dataset.git\">https://github.com/hyzhan/make_own_dataset.git</a></p>\n<p>非常感谢tzutalin提供的标注工具 <a href=\"https://github.com/tzutalin/labelImg\">github</a></p>\n<p>Thanks to tzutalin.</p>\n<h1 id=\"LabelImg\"><a href=\"#LabelImg\" class=\"headerlink\" title=\"LabelImg\"></a>LabelImg</h1><p><a href=\"https://travis-ci.org/tzutalin/labelImg\"><img src=\"https://travis-ci.org/tzutalin/labelImg.png\" alt=\"Build Status\"></a></p>\n<p>LabelImg is a graphical image annotation tool.</p>\n<p>It is written in Python and uses Qt for its graphical interface.</p>\n<p>The annotation file will be saved as an XML file. The annotation format is PASCAL VOC format, and the format is the same as <a href=\"http://www.image-net.org/\">ImageNet</a></p>\n<h2 id=\"Dependencies\"><a href=\"#Dependencies\" class=\"headerlink\" title=\"Dependencies\"></a>Dependencies</h2><ul>\n<li>Linux/Ubuntu/Mac</li>\n</ul>\n<p>Requires at least <a href=\"http://www.python.org/getit/\">Python 2.6</a> and has been tested with <a href=\"http://www.riverbankcomputing.co.uk/software/pyqt/intro\">PyQt<br>4.8</a>.</p>\n<p>In order to build the resource and assets, you need to install pyqt4-dev-tools and lxml:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo apt-get install pyqt4-dev-tools</div><div class=\"line\">$ sudo pip install lxml</div><div class=\"line\">$ make all</div><div class=\"line\">$ ./labelImg.py</div></pre></td></tr></table></figure></p>\n<p>Mac requires “$ brew install libxml2” when installing lxml</p>\n<ul>\n<li>Windows</li>\n</ul>\n<p>Need to download and setup <a href=\"https://www.python.org/downloads/windows/\">Python 2.6</a> or later and <a href=\"https://www.riverbankcomputing.com/software/pyqt/download\">PyQt4</a>. Also, you need to install other python dependencies.</p>\n<p>Open cmd and go to [labelImg]</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ pyrcc4 -o resources.py resources.qrc</div><div class=\"line\">$ python labelImg.py</div></pre></td></tr></table></figure>\n<h2 id=\"Usage\"><a href=\"#Usage\" class=\"headerlink\" title=\"Usage\"></a>Usage</h2><p>After cloning the code, you should run <code>$ make all</code> to generate the resource file.</p>\n<p>You can then start annotating by running <code>$ ./labelImg.py</code>. For usage<br>instructions you can see <a href=\"https://youtu.be/p0nR2YsCY_U\">Here</a></p>\n<p>At the moment annotations are saved as an XML file. The format is PASCAL VOC format, and the format is the same as <a href=\"http://www.image-net.org/\">ImageNet</a></p>\n<p>You can also see <a href=\"https://github.com/tzutalin/ImageNet_Utils\">ImageNet Utils</a> to download image, create a label text for machine learning, etc</p>\n<h3 id=\"General-steps-from-scratch\"><a href=\"#General-steps-from-scratch\" class=\"headerlink\" title=\"General steps from scratch\"></a>General steps from scratch</h3><ul>\n<li><p>Build and launch: <code>$ make all; python labelImg.py</code></p>\n</li>\n<li><p>Click ‘Change default saved annotation folder’ in Menu/File</p>\n</li>\n<li><p>Click ‘Open Dir’</p>\n</li>\n<li><p>Click ‘Create RectBox’</p>\n</li>\n</ul>\n<p>The annotation will be saved to the folder you specify</p>\n<h3 id=\"Create-pre-defined-classes\"><a href=\"#Create-pre-defined-classes\" class=\"headerlink\" title=\"Create pre-defined classes\"></a>Create pre-defined classes</h3><p>You can edit the <a href=\"https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt\">data/predefined_classes.txt</a> to load pre-defined classes</p>\n<h3 id=\"Hotkeys\"><a href=\"#Hotkeys\" class=\"headerlink\" title=\"Hotkeys\"></a>Hotkeys</h3><ul>\n<li><p>Ctrl + r : Change the defult target dir which saving annotation files</p>\n</li>\n<li><p>Ctrl + n : Create a bounding box</p>\n</li>\n<li><p>Ctrl + s : Save</p>\n</li>\n<li><p>n : Next image</p>\n</li>\n<li><p>p : Previous image</p>\n</li>\n</ul>\n<h3 id=\"How-to-contribute\"><a href=\"#How-to-contribute\" class=\"headerlink\" title=\"How to contribute\"></a>How to contribute</h3><p>Send a pull request</p>\n<h3 id=\"License\"><a href=\"#License\" class=\"headerlink\" title=\"License\"></a>License</h3><p><a href=\"LICENSE.md\">License</a></p>\n<p>(1).安装依赖库<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ sudo apt-get install pyqt4-dev-tools</div><div class=\"line\">$ sudo pip install lxml</div><div class=\"line\">$ make all</div></pre></td></tr></table></figure></p>\n<p>(2).图片名称批量修改<br>    将图片名称统一后方便后期工作,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python rename_images.py</div></pre></td></tr></table></figure></p>\n<p>默认图片存放路径是在JPEGImages下,执行成功后会在该文件夹下生成tmp文件夹,里面有重命名后<br>的图片文件,备份或删除原图片,在JPEGImages下仅保留重命名后的图片文件</p>\n<p>(3). 修改标签文件</p>\n<p>修改data文件下的predefined_classes.txt文件,改成自己所需要分类的类别名称,限英文</p>\n<p>(4).执行标注程序<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">./labelImg.py</div></pre></td></tr></table></figure></p>\n<p>PS.快捷键</p>\n<ul>\n<li><p>Ctrl + r : Change the defult target dir which saving annotation files</p>\n</li>\n<li><p>Ctrl + n : Create a bounding box</p>\n</li>\n<li><p>Ctrl + s : Save</p>\n</li>\n<li><p>n : Next image</p>\n</li>\n<li><p>p : Previous image</p>\n</li>\n</ul>\n<p>建议用opendir打开图片所在文件夹后再按Ctrl + r选择保存xml文件的位置(建议放在xml文件夹下),<br>以免与图片混合起来,方便后期工作.</p>\n<p>(5). 格式化xml文件(可选)</p>\n<p>部分机器会在生成的xml文件加上版本号,后期训练时需要将生成的xml文件的首行&lt;?xml version=”1.0” ?&gt;去除,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python delete_file_firstRow.py</div></pre></td></tr></table></figure></p>\n<p>执行成功后会在该文件夹下生成Annotations文件夹,里面有格式化后的xml文件</p>\n<p>(6). 分割数据集</p>\n<p>训练时需要有训练数据集,测试数据集等txt文件,执行:<br>    <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">python create_trainval.py</div></pre></td></tr></table></figure></p>\n<p>默认图片存放路径是在JPEGImages下,执行成功后会在生成ImageSets/Main/文件夹,里面有<br>四个txt文件test.txt,train.txt,trainval.txt,val.txt</p>\n<p>(7).汇总数据集</p>\n<p>将得到的Annotations,ImageSets,JPEGImages文件夹放在VOC2007文件夹(没有则新建一个)即可</p>"},{"layout":"post","title":"用faster-rcnn训练自己的数据集(VOC2007格式,python版)","date":"2017-02-21T03:24:00.000Z","description":"用faster-rcnn训练自己的数据集","_content":"\n用faster-rcnn训练自己的数据集(VOC2007格式,python版)\n\n<!--more-->\n\n## 一. 配置caffe环境\n\n[ubunt16.04下caffe环境安装](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n\n## 二. 下载,编译及测试py-faster-rcnn源码\n\n### (一)下载源码\n\n[github链接](https://github.com/rbgirshick/py-faster-rcnn)\n\n或者执行 git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git\n\n注意加上--recursive关键字\n\n### (二)编译源码\n\n编译过程中可能会出现缺失一些python模块,按提示安装\n\n#### (1)编译Cython模块\n\n\tcd $FRCN_ROOT/lib \n\tmake\n\n#### (2)修改Markfile配置\n参考[ubunt16.04下caffe环境安装](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n中修改Makefile.config\n\n#### (3)编译python接口\n\n\tcd $FRCN_ROOT/caffe-fast-rcnn\n\tmake -j8  多核编译,时间较长\n\tmake pycaffe\n\n#### (4)下载训练好的VGG16和ZF模型\n\n\tcd $FRCN_ROOT\n\t./data/scripts/fetch_faster_rcnn_models.sh\n\n时间太长的话可以考虑找网上别人分享的资源\n\n### (三)测试源码\n\n\tcd $FRCN_ROOT\n\t./tool/demo.py\n\n## 三. 使用faster-rcnn训练自己的数据集\n\n### (一)下载预训练参数及模型\n\n\tcd $FRCN_ROOT\n\t./data/scripts/fetch_imagenet_models.sh\n\t./data/scripts/fetch_selective_search_data.sh\n\n### (二)制作数据集\n\n[制作数据集(VOC2007格式)](http://report.opsauto.cn/deep%20learning/2017/02/08/%E5%88%B6%E4%BD%9C%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86.html)\n\n将制作好的VOC2007文件夹放置在data/VOCdevkit2007文件夹下,没有则新建VOCdevkit2007文件夹\n\n### (三)修改配置文件\n\n#### (1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件\n\n备注:3处修改及其附近的代码\n\t\n\tname: \"ZF\"\n\tlayer {\n\t  name: 'data'\n\t  type: 'Python'\n \t top: 'data'\n  \ttop: 'rois'\n  \ttop: 'labels'\n \t top: 'bbox_targets'\n \t top: 'bbox_inside_weights'\n  \ttop: 'bbox_outside_weights'\n  \tpython_param {\n  \t  module: 'roi_data_layer.layer'\n  \t  layer: 'RoIDataLayer'\n  \t  param_str: \"'num_classes': 2\" #按训练集类别改，该值为类别数+1\n  \t}\n\t}\n\n\tlayer {\n \t name: \"cls_score\"\n \t type: \"InnerProduct\"\n \t bottom: \"fc7\"\n \t top: \"cls_score\"\n \t param { lr_mult: 1.0 }\n \t param { lr_mult: 2.0 }\n  \tinner_product_param {\n    \tnum_output: 2 #按训练集类别改，该值为类别数+1\n   \t weight_filler {\n   \t   type: \"gaussian\"\n   \t   std: 0.01\n   \t }\n   \t bias_filler {\n  \t    type: \"constant\"\n   \t   value: 0\n  \t  }\n \t }\n\t}\n\n\tlayer {\n\t  name: \"bbox_pred\"\n \t type: \"InnerProduct\"\n \t bottom: \"fc7\"\n  \ttop: \"bbox_pred\"\n \t param { lr_mult: 1.0 }\n \t param { lr_mult: 2.0 }\n \t inner_product_param {\n \t   num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n \t   weight_filler {\n \t     type: \"gaussian\"\n\t      std: 0.001\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\n\n#### (2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件\n\n备注:1处修改及其附近的代码\n\n\n\tlayer {\n\t  name: 'input-data'\n\t  type: 'Python'\n\t  top: 'data'\n\t  top: 'im_info'\n\t  top: 'gt_boxes'\n\t  python_param {\n\t    module: 'roi_data_layer.layer'\n\t    layer: 'RoIDataLayer'\n\t    param_str: \"'num_classes': 2\" #按训练集类别改，该值为类别数+1\n\t  }\n\t}\n\n#### (3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件\n\n备注:2处修改及其附近的代码\n\n\tlayer {\n\t  name: \"cls_score\"\n\t  type: \"InnerProduct\"\n\t  bottom: \"fc7\"\n\t  top: \"cls_score\"\n\t  param { lr_mult: 1.0 }\n\t  param { lr_mult: 2.0 }\n\t  inner_product_param {\n\t    num_output: 2 #按训练集类别改，该值为类别数+1\n\t    weight_filler {\n\t      type: \"gaussian\"\n\t      std: 0.01\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\t\n\tlayer {\n\t  name: \"bbox_pred\"\n\t  type: \"InnerProduct\"\n\t  bottom: \"fc7\"\n\t  top: \"bbox_pred\"\n\t  param { lr_mult: 1.0 }\n\t  param { lr_mult: 2.0 }\n\t  inner_product_param {\n\t    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n\t    weight_filler {\n\t      type: \"gaussian\"\n\t      std: 0.001\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\n\n#### (4)修改py-faster-rcnn/lib/datasets/pascal_voc.py\n\n\n\tself._classes = ('__background__', # always index 0\n\t                         '你的标签1','你的标签2',你的标签3','你的标签4')\n\n\t注:如果只是在原始检测的20种类别:'aeroplane', 'bicycle', 'bird', 'boat','bottle', 'bus', 'car', 'cat', 'chair',\n\t'cow', 'diningtable', 'dog', 'horse','motorbike', 'person', 'pottedplant',\n\t'sheep', 'sofa', 'train', 'tvmonitor'中检测单一类别,可参考修改下面的代码:\n\n\n\tdef _load_image_set_index(self):\n\t        \"\"\"\n\t        Load the indexes listed in this dataset's image set file.\n\t        \"\"\"\n\t        # Example path to image set file:\n\t        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n\t        image_set_file = os.path.join(self._data_path, 'ImageSets', 'Main',\n\t                                      self._image_set + '.txt')\n\t        assert os.path.exists(image_set_file), \\\n\t                'Path does not exist: {}'.format(image_set_file)\n\t        with open(image_set_file) as f:\n\t            image_index = [x.strip() for x in f.readlines()]\n\t\n\t注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n\t        # only load index with cars obj\n\t        new_image_index = []\n\t        for index in image_index:\n\t            filename = os.path.join(self._data_path, 'Annotations', index + '.xml')\n\t            tree = ET.parse(filename)\n\t            objs = tree.findall('object')\n\t            num_objs = 0\n\t            for ix, obj in enumerate(objs):\n\t                curr_name = obj.find('name').text.lower().strip()\n\t                if curr_name == 'car':\n\t                    num_objs += 1\n\t                    break\n\t            if num_objs > 0:\n\t                new_image_index.append(index)\n\t        return new_image_index\n\t\n\tdef _load_pascal_annotation(self, index):\n\t        \"\"\"\n\t        Load image and bounding boxes info from XML file in the PASCAL VOC\n\t        format.\n\t        \"\"\"\n\t        filename = os.path.join(self._data_path, 'Annotations', index + '.xml')\n\t        tree = ET.parse(filename)\n\t        objs = tree.findall('object')\n\t        if not self.config['use_diff']:\n\t            # Exclude the samples labeled as difficult\n\t            non_diff_objs = [\n\t                obj for obj in objs if int(obj.find('difficult').text) == 0]\n\t            # if len(non_diff_objs) != len(objs):\n\t            #     print 'Removed {} difficult objects'.format(\n\t            #         len(objs) - len(non_diff_objs))\n\t            objs = non_diff_objs\n\t\n\t注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n\t        # change num objs , only read car\n\t        # num_objs = len(objs)\n\t\n\t        num_objs = 0\n\t        for ix, obj in enumerate(objs):\n\t            curr_name = obj.find('name').text.lower().strip()\n\t            if curr_name == 'car':\n\t                num_objs += 1\n\t\n\t        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n\t        gt_classes = np.zeros((num_objs), dtype=np.int32)\n\t        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\t        # \"Seg\" area for pascal is just the box area\n\t        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\t\n\t#注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码\n\t# Load object bounding boxes into a data \tframe.\n\t        tmp_ix = 0\n\t        for ix, obj in enumerate(objs):\n\t            bbox = obj.find('bndbox')\n\t            # Make pixel indexes 0-based\n\t            x1 = float(bbox.find('xmin').text) - 1\n\t            y1 = float(bbox.find('ymin').text) - 1\n\t            x2 = float(bbox.find('xmax').text) - 1\n\t            y2 = float(bbox.find('ymax').text) - 1\n\t            curr_name = obj.find('name').text.lower().strip()\n\t            if curr_name != 'car':\n\t                continue\n\t            cls = self._class_to_ind[curr_name]\n\t            boxes[tmp_ix, :] = [x1, y1, x2, y2]\n\t            gt_classes[tmp_ix] = cls\n\t            overlaps[tmp_ix, cls] = 1.0\n\t            seg_areas[tmp_ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\t\n\t            tmp_ix += 1\n\t\n\t        overlaps = scipy.sparse.csr_matrix(overlaps)\n\t\n\t        return {'boxes' : boxes,\n\t                'gt_classes': gt_classes,\n\t                'gt_overlaps' : overlaps,\n\t                'flipped' : False,\n\t                'seg_areas' : seg_areas}\n\n#### (4)py-faster-rcnn/lib/datasets/imdb.py修改\n\n\n\tdef append_flipped_images(self):\n\t        num_images = self.num_images\n\t        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n\t                  for i in xrange(num_images)]\n\t        for i in xrange(num_images):\n\t            boxes = self.roidb[i]['boxes'].copy()\n\t            oldx1 = boxes[:, 0].copy()\n\t            oldx2 = boxes[:, 2].copy()\n\t            boxes[:, 0] = widths[i] - oldx2 - 1\n\t            boxes[:, 2] = widths[i] - oldx1 - 1\n\t\n\t            for b in range(len(boxes)):\n\t                if boxes[b][2] < boxes[b][0]:\n\t                   boxes[b][0] = 0\n\t\n\t            assert (boxes[:, 2] >= boxes[:, 0]).all()\n\n\n#### (5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）\n\n\tmax_iters=[8000,4000,8000,4000]\n\t建议:第一次训练使用较低的迭代次数,先确保能正常训练,如max_iters=[8,4,8,4]\n\n训练分别为4个阶段（rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段）的迭代次数。可改成你希望的迭代次数。\n如果改了这些数值，最好把py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt里对应的solver文件（有4个）也修改，stepsize小于上面修改的数值，stepsize的意义是经过stepsize次的迭代后降低一次学习率（非必要修改）。\n\n#### (6)删除缓存文件(每次修改配置文件后训练都要做)\n\n\t删除py-faster-rcnn文件夹下所有的.pyc文件及data文件夹下的cache文件夹,\n\tdata/VOCdekit2007下的annotations_cache文件夹(最近一次成功训练的\n\tannotation和当前annotation一样的话这部分可以不删,否则可以正常训练,\n\t但是最后评价模型会出错)\n\n### (四)开始训练\n\n\tcd $FRCN_ROOT\n\t./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\n\n\n成功训练后在py-faster-rcnn/output/faster_rcnn_alt_opt/voc_2007_trainval文件夹下\n会有以final.caffemodel结尾的模型文件,一般为ZF_faster_rcnn_final.caffemodel\n\n成功训练后会有一次模型性能的评估测试,成功的话会有MAP指标和平均MAP指标的输出,类似下文,\n训练日志文件保存在experiments/logs文件夹下.\n\n\tEvaluating detections\n\tWriting car VOC results file\n\tVOC07 metric? Yes\n\tAP for car = 0.0090\n\tMean AP = 0.0090\n\t~~~~~~~~\n\tResults:\n\t0.009\n\t0.009\n\t~~~~~~~~\n\t\n\t--------------------------------------------------------------\n\tResults computed with the **unofficial** Python eval code.\n\tResults should be very close to the official MATLAB eval code.\n\tRecompute with `./tools/reval.py --matlab ...` for your paper.\n\t-- Thanks, The Management\n\t--------------------------------------------------------------\n\t\n\treal\t1m43.822s\n\tuser\t1m25.764s\n\tsys\t0m15.516s\n\n\t\n### (五)测试训练结果\n\n#### (1)修改py-faster-rcnn\\tools\\demo.py\n\n\tCLASSES = ('__background__',\n\t         '你的标签1','你的标签2',你的标签3','你的标签4')\n\t         \n\tNETS = {'vgg16': ('VGG16',\n\t                  'VGG16_faster_rcnn_final.caffemodel'),\n\t        'zf': ('ZF',\n\t                  'ZF_faster_rcnn_final.caffemodel')}\n\t                  \n\tim_names = os.listdir(os.path.join(cfg.DATA_DIR, 'demo'))  \n\n\n#### (2)放置模型及测试图片\n\n\t将训练得到的py-faster-rcnn\\output\\faster_rcnn_alt_opt\\***_trainval中\n\tZF的final.caffemodel拷贝至py-faster-rcnn\\data\\faster_rcnn_models\n\n\t测试图片放在py-faster-rcnn\\data\\demo(与上面demo.py设置路径有关,可修改)\n\n\n#### (3)进行测试\n\n\tcd $FRCN_ROOT\n\t./tool/demo.py\n\n\n## 四. 曾出现过的bug及当时的解决方法\n\n(1) 训练时出现KeyError:'max_overlaps'  ,解决方法:删除data文件夹下的cache文件夹\n\n(2) 训练结束后测试时出现类似\n\n\tFile \"/home/hyzhan/py-faster-rcnn/tools/../lib/datasets/voc_eval.py\", line 126, in voc_eval\n\t    R = [obj for obj in recs[imagename] if obj['name'] == classname]\n\tKeyError: '000002'\n\n解决方法: 删除data/VOCdekit2007下的annotations_cache文件夹\n\n(3) caffe-fast-rcnn编译时出现找不到nvcc命令的情况,解决方法:\n\n\texport PATH=/usr/local/cuda-8.0/bin:$PATH\n\texport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\n\n将cuda安装路径添加到环境变量中\n\n(4) caffe-fast-rcnn编译时出现类似找不到opencv命令的情况,解决方法,添加环境变量:\n\n\texport LD_LIBRARY_PATH=/home/hyzhan/software/opencv3/lib:$LD_LIBRARY_PATH\n\n(5) 训练的时候执行\"./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\"语句进行训练会出现找不到faster_rcnn_alt_opt.sh文件的情况,解决方法:重新手打命令\n\n(6) 测试之前需要修改tool文件夹下的demo或者mydemo里面的class类别,不然会显示上次训练的类别\n","source":"_posts/2017-02-21-用faster-rcnn训练自己的数据集(python版).md","raw":"---\nlayout: post\ntitle: \"用faster-rcnn训练自己的数据集(VOC2007格式,python版)\"\ndate: 2017-02-21 11:24:00\ndescription: \"用faster-rcnn训练自己的数据集\"\ncategory: [deep learning]\ntags: [python]\n---\n\n用faster-rcnn训练自己的数据集(VOC2007格式,python版)\n\n<!--more-->\n\n## 一. 配置caffe环境\n\n[ubunt16.04下caffe环境安装](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n\n## 二. 下载,编译及测试py-faster-rcnn源码\n\n### (一)下载源码\n\n[github链接](https://github.com/rbgirshick/py-faster-rcnn)\n\n或者执行 git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git\n\n注意加上--recursive关键字\n\n### (二)编译源码\n\n编译过程中可能会出现缺失一些python模块,按提示安装\n\n#### (1)编译Cython模块\n\n\tcd $FRCN_ROOT/lib \n\tmake\n\n#### (2)修改Markfile配置\n参考[ubunt16.04下caffe环境安装](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n中修改Makefile.config\n\n#### (3)编译python接口\n\n\tcd $FRCN_ROOT/caffe-fast-rcnn\n\tmake -j8  多核编译,时间较长\n\tmake pycaffe\n\n#### (4)下载训练好的VGG16和ZF模型\n\n\tcd $FRCN_ROOT\n\t./data/scripts/fetch_faster_rcnn_models.sh\n\n时间太长的话可以考虑找网上别人分享的资源\n\n### (三)测试源码\n\n\tcd $FRCN_ROOT\n\t./tool/demo.py\n\n## 三. 使用faster-rcnn训练自己的数据集\n\n### (一)下载预训练参数及模型\n\n\tcd $FRCN_ROOT\n\t./data/scripts/fetch_imagenet_models.sh\n\t./data/scripts/fetch_selective_search_data.sh\n\n### (二)制作数据集\n\n[制作数据集(VOC2007格式)](http://report.opsauto.cn/deep%20learning/2017/02/08/%E5%88%B6%E4%BD%9C%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86.html)\n\n将制作好的VOC2007文件夹放置在data/VOCdevkit2007文件夹下,没有则新建VOCdevkit2007文件夹\n\n### (三)修改配置文件\n\n#### (1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件\n\n备注:3处修改及其附近的代码\n\t\n\tname: \"ZF\"\n\tlayer {\n\t  name: 'data'\n\t  type: 'Python'\n \t top: 'data'\n  \ttop: 'rois'\n  \ttop: 'labels'\n \t top: 'bbox_targets'\n \t top: 'bbox_inside_weights'\n  \ttop: 'bbox_outside_weights'\n  \tpython_param {\n  \t  module: 'roi_data_layer.layer'\n  \t  layer: 'RoIDataLayer'\n  \t  param_str: \"'num_classes': 2\" #按训练集类别改，该值为类别数+1\n  \t}\n\t}\n\n\tlayer {\n \t name: \"cls_score\"\n \t type: \"InnerProduct\"\n \t bottom: \"fc7\"\n \t top: \"cls_score\"\n \t param { lr_mult: 1.0 }\n \t param { lr_mult: 2.0 }\n  \tinner_product_param {\n    \tnum_output: 2 #按训练集类别改，该值为类别数+1\n   \t weight_filler {\n   \t   type: \"gaussian\"\n   \t   std: 0.01\n   \t }\n   \t bias_filler {\n  \t    type: \"constant\"\n   \t   value: 0\n  \t  }\n \t }\n\t}\n\n\tlayer {\n\t  name: \"bbox_pred\"\n \t type: \"InnerProduct\"\n \t bottom: \"fc7\"\n  \ttop: \"bbox_pred\"\n \t param { lr_mult: 1.0 }\n \t param { lr_mult: 2.0 }\n \t inner_product_param {\n \t   num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n \t   weight_filler {\n \t     type: \"gaussian\"\n\t      std: 0.001\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\n\n#### (2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件\n\n备注:1处修改及其附近的代码\n\n\n\tlayer {\n\t  name: 'input-data'\n\t  type: 'Python'\n\t  top: 'data'\n\t  top: 'im_info'\n\t  top: 'gt_boxes'\n\t  python_param {\n\t    module: 'roi_data_layer.layer'\n\t    layer: 'RoIDataLayer'\n\t    param_str: \"'num_classes': 2\" #按训练集类别改，该值为类别数+1\n\t  }\n\t}\n\n#### (3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件\n\n备注:2处修改及其附近的代码\n\n\tlayer {\n\t  name: \"cls_score\"\n\t  type: \"InnerProduct\"\n\t  bottom: \"fc7\"\n\t  top: \"cls_score\"\n\t  param { lr_mult: 1.0 }\n\t  param { lr_mult: 2.0 }\n\t  inner_product_param {\n\t    num_output: 2 #按训练集类别改，该值为类别数+1\n\t    weight_filler {\n\t      type: \"gaussian\"\n\t      std: 0.01\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\t\n\tlayer {\n\t  name: \"bbox_pred\"\n\t  type: \"InnerProduct\"\n\t  bottom: \"fc7\"\n\t  top: \"bbox_pred\"\n\t  param { lr_mult: 1.0 }\n\t  param { lr_mult: 2.0 }\n\t  inner_product_param {\n\t    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n\t    weight_filler {\n\t      type: \"gaussian\"\n\t      std: 0.001\n\t    }\n\t    bias_filler {\n\t      type: \"constant\"\n\t      value: 0\n\t    }\n\t  }\n\t}\n\n\n#### (4)修改py-faster-rcnn/lib/datasets/pascal_voc.py\n\n\n\tself._classes = ('__background__', # always index 0\n\t                         '你的标签1','你的标签2',你的标签3','你的标签4')\n\n\t注:如果只是在原始检测的20种类别:'aeroplane', 'bicycle', 'bird', 'boat','bottle', 'bus', 'car', 'cat', 'chair',\n\t'cow', 'diningtable', 'dog', 'horse','motorbike', 'person', 'pottedplant',\n\t'sheep', 'sofa', 'train', 'tvmonitor'中检测单一类别,可参考修改下面的代码:\n\n\n\tdef _load_image_set_index(self):\n\t        \"\"\"\n\t        Load the indexes listed in this dataset's image set file.\n\t        \"\"\"\n\t        # Example path to image set file:\n\t        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n\t        image_set_file = os.path.join(self._data_path, 'ImageSets', 'Main',\n\t                                      self._image_set + '.txt')\n\t        assert os.path.exists(image_set_file), \\\n\t                'Path does not exist: {}'.format(image_set_file)\n\t        with open(image_set_file) as f:\n\t            image_index = [x.strip() for x in f.readlines()]\n\t\n\t注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n\t        # only load index with cars obj\n\t        new_image_index = []\n\t        for index in image_index:\n\t            filename = os.path.join(self._data_path, 'Annotations', index + '.xml')\n\t            tree = ET.parse(filename)\n\t            objs = tree.findall('object')\n\t            num_objs = 0\n\t            for ix, obj in enumerate(objs):\n\t                curr_name = obj.find('name').text.lower().strip()\n\t                if curr_name == 'car':\n\t                    num_objs += 1\n\t                    break\n\t            if num_objs > 0:\n\t                new_image_index.append(index)\n\t        return new_image_index\n\t\n\tdef _load_pascal_annotation(self, index):\n\t        \"\"\"\n\t        Load image and bounding boxes info from XML file in the PASCAL VOC\n\t        format.\n\t        \"\"\"\n\t        filename = os.path.join(self._data_path, 'Annotations', index + '.xml')\n\t        tree = ET.parse(filename)\n\t        objs = tree.findall('object')\n\t        if not self.config['use_diff']:\n\t            # Exclude the samples labeled as difficult\n\t            non_diff_objs = [\n\t                obj for obj in objs if int(obj.find('difficult').text) == 0]\n\t            # if len(non_diff_objs) != len(objs):\n\t            #     print 'Removed {} difficult objects'.format(\n\t            #         len(objs) - len(non_diff_objs))\n\t            objs = non_diff_objs\n\t\n\t注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n\t        # change num objs , only read car\n\t        # num_objs = len(objs)\n\t\n\t        num_objs = 0\n\t        for ix, obj in enumerate(objs):\n\t            curr_name = obj.find('name').text.lower().strip()\n\t            if curr_name == 'car':\n\t                num_objs += 1\n\t\n\t        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n\t        gt_classes = np.zeros((num_objs), dtype=np.int32)\n\t        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\t        # \"Seg\" area for pascal is just the box area\n\t        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\t\n\t#注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码\n\t# Load object bounding boxes into a data \tframe.\n\t        tmp_ix = 0\n\t        for ix, obj in enumerate(objs):\n\t            bbox = obj.find('bndbox')\n\t            # Make pixel indexes 0-based\n\t            x1 = float(bbox.find('xmin').text) - 1\n\t            y1 = float(bbox.find('ymin').text) - 1\n\t            x2 = float(bbox.find('xmax').text) - 1\n\t            y2 = float(bbox.find('ymax').text) - 1\n\t            curr_name = obj.find('name').text.lower().strip()\n\t            if curr_name != 'car':\n\t                continue\n\t            cls = self._class_to_ind[curr_name]\n\t            boxes[tmp_ix, :] = [x1, y1, x2, y2]\n\t            gt_classes[tmp_ix] = cls\n\t            overlaps[tmp_ix, cls] = 1.0\n\t            seg_areas[tmp_ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\t\n\t            tmp_ix += 1\n\t\n\t        overlaps = scipy.sparse.csr_matrix(overlaps)\n\t\n\t        return {'boxes' : boxes,\n\t                'gt_classes': gt_classes,\n\t                'gt_overlaps' : overlaps,\n\t                'flipped' : False,\n\t                'seg_areas' : seg_areas}\n\n#### (4)py-faster-rcnn/lib/datasets/imdb.py修改\n\n\n\tdef append_flipped_images(self):\n\t        num_images = self.num_images\n\t        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n\t                  for i in xrange(num_images)]\n\t        for i in xrange(num_images):\n\t            boxes = self.roidb[i]['boxes'].copy()\n\t            oldx1 = boxes[:, 0].copy()\n\t            oldx2 = boxes[:, 2].copy()\n\t            boxes[:, 0] = widths[i] - oldx2 - 1\n\t            boxes[:, 2] = widths[i] - oldx1 - 1\n\t\n\t            for b in range(len(boxes)):\n\t                if boxes[b][2] < boxes[b][0]:\n\t                   boxes[b][0] = 0\n\t\n\t            assert (boxes[:, 2] >= boxes[:, 0]).all()\n\n\n#### (5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）\n\n\tmax_iters=[8000,4000,8000,4000]\n\t建议:第一次训练使用较低的迭代次数,先确保能正常训练,如max_iters=[8,4,8,4]\n\n训练分别为4个阶段（rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段）的迭代次数。可改成你希望的迭代次数。\n如果改了这些数值，最好把py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt里对应的solver文件（有4个）也修改，stepsize小于上面修改的数值，stepsize的意义是经过stepsize次的迭代后降低一次学习率（非必要修改）。\n\n#### (6)删除缓存文件(每次修改配置文件后训练都要做)\n\n\t删除py-faster-rcnn文件夹下所有的.pyc文件及data文件夹下的cache文件夹,\n\tdata/VOCdekit2007下的annotations_cache文件夹(最近一次成功训练的\n\tannotation和当前annotation一样的话这部分可以不删,否则可以正常训练,\n\t但是最后评价模型会出错)\n\n### (四)开始训练\n\n\tcd $FRCN_ROOT\n\t./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\n\n\n成功训练后在py-faster-rcnn/output/faster_rcnn_alt_opt/voc_2007_trainval文件夹下\n会有以final.caffemodel结尾的模型文件,一般为ZF_faster_rcnn_final.caffemodel\n\n成功训练后会有一次模型性能的评估测试,成功的话会有MAP指标和平均MAP指标的输出,类似下文,\n训练日志文件保存在experiments/logs文件夹下.\n\n\tEvaluating detections\n\tWriting car VOC results file\n\tVOC07 metric? Yes\n\tAP for car = 0.0090\n\tMean AP = 0.0090\n\t~~~~~~~~\n\tResults:\n\t0.009\n\t0.009\n\t~~~~~~~~\n\t\n\t--------------------------------------------------------------\n\tResults computed with the **unofficial** Python eval code.\n\tResults should be very close to the official MATLAB eval code.\n\tRecompute with `./tools/reval.py --matlab ...` for your paper.\n\t-- Thanks, The Management\n\t--------------------------------------------------------------\n\t\n\treal\t1m43.822s\n\tuser\t1m25.764s\n\tsys\t0m15.516s\n\n\t\n### (五)测试训练结果\n\n#### (1)修改py-faster-rcnn\\tools\\demo.py\n\n\tCLASSES = ('__background__',\n\t         '你的标签1','你的标签2',你的标签3','你的标签4')\n\t         \n\tNETS = {'vgg16': ('VGG16',\n\t                  'VGG16_faster_rcnn_final.caffemodel'),\n\t        'zf': ('ZF',\n\t                  'ZF_faster_rcnn_final.caffemodel')}\n\t                  \n\tim_names = os.listdir(os.path.join(cfg.DATA_DIR, 'demo'))  \n\n\n#### (2)放置模型及测试图片\n\n\t将训练得到的py-faster-rcnn\\output\\faster_rcnn_alt_opt\\***_trainval中\n\tZF的final.caffemodel拷贝至py-faster-rcnn\\data\\faster_rcnn_models\n\n\t测试图片放在py-faster-rcnn\\data\\demo(与上面demo.py设置路径有关,可修改)\n\n\n#### (3)进行测试\n\n\tcd $FRCN_ROOT\n\t./tool/demo.py\n\n\n## 四. 曾出现过的bug及当时的解决方法\n\n(1) 训练时出现KeyError:'max_overlaps'  ,解决方法:删除data文件夹下的cache文件夹\n\n(2) 训练结束后测试时出现类似\n\n\tFile \"/home/hyzhan/py-faster-rcnn/tools/../lib/datasets/voc_eval.py\", line 126, in voc_eval\n\t    R = [obj for obj in recs[imagename] if obj['name'] == classname]\n\tKeyError: '000002'\n\n解决方法: 删除data/VOCdekit2007下的annotations_cache文件夹\n\n(3) caffe-fast-rcnn编译时出现找不到nvcc命令的情况,解决方法:\n\n\texport PATH=/usr/local/cuda-8.0/bin:$PATH\n\texport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\n\n将cuda安装路径添加到环境变量中\n\n(4) caffe-fast-rcnn编译时出现类似找不到opencv命令的情况,解决方法,添加环境变量:\n\n\texport LD_LIBRARY_PATH=/home/hyzhan/software/opencv3/lib:$LD_LIBRARY_PATH\n\n(5) 训练的时候执行\"./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\"语句进行训练会出现找不到faster_rcnn_alt_opt.sh文件的情况,解决方法:重新手打命令\n\n(6) 测试之前需要修改tool文件夹下的demo或者mydemo里面的class类别,不然会显示上次训练的类别\n","slug":"2017-02-21-用faster-rcnn训练自己的数据集(python版)","published":1,"updated":"2017-04-18T05:45:39.191Z","comments":1,"photos":[],"link":"","_id":"cj2ycuxiy0006tdjc7kpm2in9","content":"<p>用faster-rcnn训练自己的数据集(VOC2007格式,python版)</p>\n<a id=\"more\"></a>\n<h2 id=\"一-配置caffe环境\"><a href=\"#一-配置caffe环境\" class=\"headerlink\" title=\"一. 配置caffe环境\"></a>一. 配置caffe环境</h2><p><a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\" target=\"_blank\" rel=\"external\">ubunt16.04下caffe环境安装</a></p>\n<h2 id=\"二-下载-编译及测试py-faster-rcnn源码\"><a href=\"#二-下载-编译及测试py-faster-rcnn源码\" class=\"headerlink\" title=\"二. 下载,编译及测试py-faster-rcnn源码\"></a>二. 下载,编译及测试py-faster-rcnn源码</h2><h3 id=\"一-下载源码\"><a href=\"#一-下载源码\" class=\"headerlink\" title=\"(一)下载源码\"></a>(一)下载源码</h3><p><a href=\"https://github.com/rbgirshick/py-faster-rcnn\" target=\"_blank\" rel=\"external\">github链接</a></p>\n<p>或者执行 git clone –recursive <a href=\"https://github.com/rbgirshick/py-faster-rcnn.git\" target=\"_blank\" rel=\"external\">https://github.com/rbgirshick/py-faster-rcnn.git</a></p>\n<p>注意加上–recursive关键字</p>\n<h3 id=\"二-编译源码\"><a href=\"#二-编译源码\" class=\"headerlink\" title=\"(二)编译源码\"></a>(二)编译源码</h3><p>编译过程中可能会出现缺失一些python模块,按提示安装</p>\n<h4 id=\"1-编译Cython模块\"><a href=\"#1-编译Cython模块\" class=\"headerlink\" title=\"(1)编译Cython模块\"></a>(1)编译Cython模块</h4><pre><code>cd $FRCN_ROOT/lib \nmake\n</code></pre><h4 id=\"2-修改Markfile配置\"><a href=\"#2-修改Markfile配置\" class=\"headerlink\" title=\"(2)修改Markfile配置\"></a>(2)修改Markfile配置</h4><p>参考<a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\" target=\"_blank\" rel=\"external\">ubunt16.04下caffe环境安装</a><br>中修改Makefile.config</p>\n<h4 id=\"3-编译python接口\"><a href=\"#3-编译python接口\" class=\"headerlink\" title=\"(3)编译python接口\"></a>(3)编译python接口</h4><pre><code>cd $FRCN_ROOT/caffe-fast-rcnn\nmake -j8  多核编译,时间较长\nmake pycaffe\n</code></pre><h4 id=\"4-下载训练好的VGG16和ZF模型\"><a href=\"#4-下载训练好的VGG16和ZF模型\" class=\"headerlink\" title=\"(4)下载训练好的VGG16和ZF模型\"></a>(4)下载训练好的VGG16和ZF模型</h4><pre><code>cd $FRCN_ROOT\n./data/scripts/fetch_faster_rcnn_models.sh\n</code></pre><p>时间太长的话可以考虑找网上别人分享的资源</p>\n<h3 id=\"三-测试源码\"><a href=\"#三-测试源码\" class=\"headerlink\" title=\"(三)测试源码\"></a>(三)测试源码</h3><pre><code>cd $FRCN_ROOT\n./tool/demo.py\n</code></pre><h2 id=\"三-使用faster-rcnn训练自己的数据集\"><a href=\"#三-使用faster-rcnn训练自己的数据集\" class=\"headerlink\" title=\"三. 使用faster-rcnn训练自己的数据集\"></a>三. 使用faster-rcnn训练自己的数据集</h2><h3 id=\"一-下载预训练参数及模型\"><a href=\"#一-下载预训练参数及模型\" class=\"headerlink\" title=\"(一)下载预训练参数及模型\"></a>(一)下载预训练参数及模型</h3><pre><code>cd $FRCN_ROOT\n./data/scripts/fetch_imagenet_models.sh\n./data/scripts/fetch_selective_search_data.sh\n</code></pre><h3 id=\"二-制作数据集\"><a href=\"#二-制作数据集\" class=\"headerlink\" title=\"(二)制作数据集\"></a>(二)制作数据集</h3><p><a href=\"http://report.opsauto.cn/deep%20learning/2017/02/08/%E5%88%B6%E4%BD%9C%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86.html\" target=\"_blank\" rel=\"external\">制作数据集(VOC2007格式)</a></p>\n<p>将制作好的VOC2007文件夹放置在data/VOCdevkit2007文件夹下,没有则新建VOCdevkit2007文件夹</p>\n<h3 id=\"三-修改配置文件\"><a href=\"#三-修改配置文件\" class=\"headerlink\" title=\"(三)修改配置文件\"></a>(三)修改配置文件</h3><h4 id=\"1-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-fast-rcnn-train-pt和stage2-fast-rcnn-train-pt-两个文件\"><a href=\"#1-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-fast-rcnn-train-pt和stage2-fast-rcnn-train-pt-两个文件\" class=\"headerlink\" title=\"(1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件\"></a>(1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件</h4><p>备注:3处修改及其附近的代码</p>\n<pre><code>name: &quot;ZF&quot;\nlayer {\n  name: &apos;data&apos;\n  type: &apos;Python&apos;\n  top: &apos;data&apos;\n  top: &apos;rois&apos;\n  top: &apos;labels&apos;\n  top: &apos;bbox_targets&apos;\n  top: &apos;bbox_inside_weights&apos;\n  top: &apos;bbox_outside_weights&apos;\n  python_param {\n    module: &apos;roi_data_layer.layer&apos;\n    layer: &apos;RoIDataLayer&apos;\n    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1\n  }\n}\n\nlayer {\n  name: &quot;cls_score&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;cls_score&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 2 #按训练集类别改，该值为类别数+1\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.01\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n\nlayer {\n  name: &quot;bbox_pred&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;bbox_pred&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.001\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n</code></pre><h4 id=\"2-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-rpn-train-pt和stage2-rpn-train-pt-两个文件\"><a href=\"#2-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-rpn-train-pt和stage2-rpn-train-pt-两个文件\" class=\"headerlink\" title=\"(2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件\"></a>(2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件</h4><p>备注:1处修改及其附近的代码</p>\n<pre><code>layer {\n  name: &apos;input-data&apos;\n  type: &apos;Python&apos;\n  top: &apos;data&apos;\n  top: &apos;im_info&apos;\n  top: &apos;gt_boxes&apos;\n  python_param {\n    module: &apos;roi_data_layer.layer&apos;\n    layer: &apos;RoIDataLayer&apos;\n    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1\n  }\n}\n</code></pre><h4 id=\"3-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-faster-rcnn-test-pt文件\"><a href=\"#3-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-faster-rcnn-test-pt文件\" class=\"headerlink\" title=\"(3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件\"></a>(3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件</h4><p>备注:2处修改及其附近的代码</p>\n<pre><code>layer {\n  name: &quot;cls_score&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;cls_score&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 2 #按训练集类别改，该值为类别数+1\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.01\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n\nlayer {\n  name: &quot;bbox_pred&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;bbox_pred&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.001\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n</code></pre><h4 id=\"4-修改py-faster-rcnn-lib-datasets-pascal-voc-py\"><a href=\"#4-修改py-faster-rcnn-lib-datasets-pascal-voc-py\" class=\"headerlink\" title=\"(4)修改py-faster-rcnn/lib/datasets/pascal_voc.py\"></a>(4)修改py-faster-rcnn/lib/datasets/pascal_voc.py</h4><pre><code>self._classes = (&apos;__background__&apos;, # always index 0\n                         &apos;你的标签1&apos;,&apos;你的标签2&apos;,你的标签3&apos;,&apos;你的标签4&apos;)\n\n注:如果只是在原始检测的20种类别:&apos;aeroplane&apos;, &apos;bicycle&apos;, &apos;bird&apos;, &apos;boat&apos;,&apos;bottle&apos;, &apos;bus&apos;, &apos;car&apos;, &apos;cat&apos;, &apos;chair&apos;,\n&apos;cow&apos;, &apos;diningtable&apos;, &apos;dog&apos;, &apos;horse&apos;,&apos;motorbike&apos;, &apos;person&apos;, &apos;pottedplant&apos;,\n&apos;sheep&apos;, &apos;sofa&apos;, &apos;train&apos;, &apos;tvmonitor&apos;中检测单一类别,可参考修改下面的代码:\n\n\ndef _load_image_set_index(self):\n        &quot;&quot;&quot;\n        Load the indexes listed in this dataset&apos;s image set file.\n        &quot;&quot;&quot;\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, &apos;ImageSets&apos;, &apos;Main&apos;,\n                                      self._image_set + &apos;.txt&apos;)\n        assert os.path.exists(image_set_file), \\\n                &apos;Path does not exist: {}&apos;.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n\n注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n        # only load index with cars obj\n        new_image_index = []\n        for index in image_index:\n            filename = os.path.join(self._data_path, &apos;Annotations&apos;, index + &apos;.xml&apos;)\n            tree = ET.parse(filename)\n            objs = tree.findall(&apos;object&apos;)\n            num_objs = 0\n            for ix, obj in enumerate(objs):\n                curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n                if curr_name == &apos;car&apos;:\n                    num_objs += 1\n                    break\n            if num_objs &gt; 0:\n                new_image_index.append(index)\n        return new_image_index\n\ndef _load_pascal_annotation(self, index):\n        &quot;&quot;&quot;\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        &quot;&quot;&quot;\n        filename = os.path.join(self._data_path, &apos;Annotations&apos;, index + &apos;.xml&apos;)\n        tree = ET.parse(filename)\n        objs = tree.findall(&apos;object&apos;)\n        if not self.config[&apos;use_diff&apos;]:\n            # Exclude the samples labeled as difficult\n            non_diff_objs = [\n                obj for obj in objs if int(obj.find(&apos;difficult&apos;).text) == 0]\n            # if len(non_diff_objs) != len(objs):\n            #     print &apos;Removed {} difficult objects&apos;.format(\n            #         len(objs) - len(non_diff_objs))\n            objs = non_diff_objs\n\n注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n        # change num objs , only read car\n        # num_objs = len(objs)\n\n        num_objs = 0\n        for ix, obj in enumerate(objs):\n            curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n            if curr_name == &apos;car&apos;:\n                num_objs += 1\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # &quot;Seg&quot; area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n#注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码\n# Load object bounding boxes into a data     frame.\n        tmp_ix = 0\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(&apos;bndbox&apos;)\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(&apos;xmin&apos;).text) - 1\n            y1 = float(bbox.find(&apos;ymin&apos;).text) - 1\n            x2 = float(bbox.find(&apos;xmax&apos;).text) - 1\n            y2 = float(bbox.find(&apos;ymax&apos;).text) - 1\n            curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n            if curr_name != &apos;car&apos;:\n                continue\n            cls = self._class_to_ind[curr_name]\n            boxes[tmp_ix, :] = [x1, y1, x2, y2]\n            gt_classes[tmp_ix] = cls\n            overlaps[tmp_ix, cls] = 1.0\n            seg_areas[tmp_ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n            tmp_ix += 1\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {&apos;boxes&apos; : boxes,\n                &apos;gt_classes&apos;: gt_classes,\n                &apos;gt_overlaps&apos; : overlaps,\n                &apos;flipped&apos; : False,\n                &apos;seg_areas&apos; : seg_areas}\n</code></pre><h4 id=\"4-py-faster-rcnn-lib-datasets-imdb-py修改\"><a href=\"#4-py-faster-rcnn-lib-datasets-imdb-py修改\" class=\"headerlink\" title=\"(4)py-faster-rcnn/lib/datasets/imdb.py修改\"></a>(4)py-faster-rcnn/lib/datasets/imdb.py修改</h4><pre><code>def append_flipped_images(self):\n        num_images = self.num_images\n        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n                  for i in xrange(num_images)]\n        for i in xrange(num_images):\n            boxes = self.roidb[i][&apos;boxes&apos;].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n\n            for b in range(len(boxes)):\n                if boxes[b][2] &lt; boxes[b][0]:\n                   boxes[b][0] = 0\n\n            assert (boxes[:, 2] &gt;= boxes[:, 0]).all()\n</code></pre><h4 id=\"5-py-faster-rcnn-tools-train-faster-rcnn-alt-opt-py修改迭代次数（建议修改）\"><a href=\"#5-py-faster-rcnn-tools-train-faster-rcnn-alt-opt-py修改迭代次数（建议修改）\" class=\"headerlink\" title=\"(5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）\"></a>(5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）</h4><pre><code>max_iters=[8000,4000,8000,4000]\n建议:第一次训练使用较低的迭代次数,先确保能正常训练,如max_iters=[8,4,8,4]\n</code></pre><p>训练分别为4个阶段（rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段）的迭代次数。可改成你希望的迭代次数。<br>如果改了这些数值，最好把py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt里对应的solver文件（有4个）也修改，stepsize小于上面修改的数值，stepsize的意义是经过stepsize次的迭代后降低一次学习率（非必要修改）。</p>\n<h4 id=\"6-删除缓存文件-每次修改配置文件后训练都要做\"><a href=\"#6-删除缓存文件-每次修改配置文件后训练都要做\" class=\"headerlink\" title=\"(6)删除缓存文件(每次修改配置文件后训练都要做)\"></a>(6)删除缓存文件(每次修改配置文件后训练都要做)</h4><pre><code>删除py-faster-rcnn文件夹下所有的.pyc文件及data文件夹下的cache文件夹,\ndata/VOCdekit2007下的annotations_cache文件夹(最近一次成功训练的\nannotation和当前annotation一样的话这部分可以不删,否则可以正常训练,\n但是最后评价模型会出错)\n</code></pre><h3 id=\"四-开始训练\"><a href=\"#四-开始训练\" class=\"headerlink\" title=\"(四)开始训练\"></a>(四)开始训练</h3><pre><code>cd $FRCN_ROOT\n./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\n</code></pre><p>成功训练后在py-faster-rcnn/output/faster_rcnn_alt_opt/voc_2007_trainval文件夹下<br>会有以final.caffemodel结尾的模型文件,一般为ZF_faster_rcnn_final.caffemodel</p>\n<p>成功训练后会有一次模型性能的评估测试,成功的话会有MAP指标和平均MAP指标的输出,类似下文,<br>训练日志文件保存在experiments/logs文件夹下.</p>\n<pre><code>Evaluating detections\nWriting car VOC results file\nVOC07 metric? Yes\nAP for car = 0.0090\nMean AP = 0.0090\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">Results:</div><div class=\"line\">0.009</div><div class=\"line\">0.009</div></pre></td></tr></table></figure>\n\n\n--------------------------------------------------------------\nResults computed with the **unofficial** Python eval code.\nResults should be very close to the official MATLAB eval code.\nRecompute with `./tools/reval.py --matlab ...` for your paper.\n-- Thanks, The Management\n--------------------------------------------------------------\n\nreal    1m43.822s\nuser    1m25.764s\nsys    0m15.516s\n</code></pre><h3 id=\"五-测试训练结果\"><a href=\"#五-测试训练结果\" class=\"headerlink\" title=\"(五)测试训练结果\"></a>(五)测试训练结果</h3><h4 id=\"1-修改py-faster-rcnn-tools-demo-py\"><a href=\"#1-修改py-faster-rcnn-tools-demo-py\" class=\"headerlink\" title=\"(1)修改py-faster-rcnn\\tools\\demo.py\"></a>(1)修改py-faster-rcnn\\tools\\demo.py</h4><pre><code>CLASSES = (&apos;__background__&apos;,\n         &apos;你的标签1&apos;,&apos;你的标签2&apos;,你的标签3&apos;,&apos;你的标签4&apos;)\n\nNETS = {&apos;vgg16&apos;: (&apos;VGG16&apos;,\n                  &apos;VGG16_faster_rcnn_final.caffemodel&apos;),\n        &apos;zf&apos;: (&apos;ZF&apos;,\n                  &apos;ZF_faster_rcnn_final.caffemodel&apos;)}\n\nim_names = os.listdir(os.path.join(cfg.DATA_DIR, &apos;demo&apos;))  \n</code></pre><h4 id=\"2-放置模型及测试图片\"><a href=\"#2-放置模型及测试图片\" class=\"headerlink\" title=\"(2)放置模型及测试图片\"></a>(2)放置模型及测试图片</h4><pre><code>将训练得到的py-faster-rcnn\\output\\faster_rcnn_alt_opt\\***_trainval中\nZF的final.caffemodel拷贝至py-faster-rcnn\\data\\faster_rcnn_models\n\n测试图片放在py-faster-rcnn\\data\\demo(与上面demo.py设置路径有关,可修改)\n</code></pre><h4 id=\"3-进行测试\"><a href=\"#3-进行测试\" class=\"headerlink\" title=\"(3)进行测试\"></a>(3)进行测试</h4><pre><code>cd $FRCN_ROOT\n./tool/demo.py\n</code></pre><h2 id=\"四-曾出现过的bug及当时的解决方法\"><a href=\"#四-曾出现过的bug及当时的解决方法\" class=\"headerlink\" title=\"四. 曾出现过的bug及当时的解决方法\"></a>四. 曾出现过的bug及当时的解决方法</h2><p>(1) 训练时出现KeyError:’max_overlaps’  ,解决方法:删除data文件夹下的cache文件夹</p>\n<p>(2) 训练结束后测试时出现类似</p>\n<pre><code>File &quot;/home/hyzhan/py-faster-rcnn/tools/../lib/datasets/voc_eval.py&quot;, line 126, in voc_eval\n    R = [obj for obj in recs[imagename] if obj[&apos;name&apos;] == classname]\nKeyError: &apos;000002&apos;\n</code></pre><p>解决方法: 删除data/VOCdekit2007下的annotations_cache文件夹</p>\n<p>(3) caffe-fast-rcnn编译时出现找不到nvcc命令的情况,解决方法:</p>\n<pre><code>export PATH=/usr/local/cuda-8.0/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\n</code></pre><p>将cuda安装路径添加到环境变量中</p>\n<p>(4) caffe-fast-rcnn编译时出现类似找不到opencv命令的情况,解决方法,添加环境变量:</p>\n<pre><code>export LD_LIBRARY_PATH=/home/hyzhan/software/opencv3/lib:$LD_LIBRARY_PATH\n</code></pre><p>(5) 训练的时候执行”./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc”语句进行训练会出现找不到faster_rcnn_alt_opt.sh文件的情况,解决方法:重新手打命令</p>\n<p>(6) 测试之前需要修改tool文件夹下的demo或者mydemo里面的class类别,不然会显示上次训练的类别</p>\n","site":{"data":{}},"excerpt":"<p>用faster-rcnn训练自己的数据集(VOC2007格式,python版)</p>","more":"<h2 id=\"一-配置caffe环境\"><a href=\"#一-配置caffe环境\" class=\"headerlink\" title=\"一. 配置caffe环境\"></a>一. 配置caffe环境</h2><p><a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\">ubunt16.04下caffe环境安装</a></p>\n<h2 id=\"二-下载-编译及测试py-faster-rcnn源码\"><a href=\"#二-下载-编译及测试py-faster-rcnn源码\" class=\"headerlink\" title=\"二. 下载,编译及测试py-faster-rcnn源码\"></a>二. 下载,编译及测试py-faster-rcnn源码</h2><h3 id=\"一-下载源码\"><a href=\"#一-下载源码\" class=\"headerlink\" title=\"(一)下载源码\"></a>(一)下载源码</h3><p><a href=\"https://github.com/rbgirshick/py-faster-rcnn\">github链接</a></p>\n<p>或者执行 git clone –recursive <a href=\"https://github.com/rbgirshick/py-faster-rcnn.git\">https://github.com/rbgirshick/py-faster-rcnn.git</a></p>\n<p>注意加上–recursive关键字</p>\n<h3 id=\"二-编译源码\"><a href=\"#二-编译源码\" class=\"headerlink\" title=\"(二)编译源码\"></a>(二)编译源码</h3><p>编译过程中可能会出现缺失一些python模块,按提示安装</p>\n<h4 id=\"1-编译Cython模块\"><a href=\"#1-编译Cython模块\" class=\"headerlink\" title=\"(1)编译Cython模块\"></a>(1)编译Cython模块</h4><pre><code>cd $FRCN_ROOT/lib \nmake\n</code></pre><h4 id=\"2-修改Markfile配置\"><a href=\"#2-修改Markfile配置\" class=\"headerlink\" title=\"(2)修改Markfile配置\"></a>(2)修改Markfile配置</h4><p>参考<a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\">ubunt16.04下caffe环境安装</a><br>中修改Makefile.config</p>\n<h4 id=\"3-编译python接口\"><a href=\"#3-编译python接口\" class=\"headerlink\" title=\"(3)编译python接口\"></a>(3)编译python接口</h4><pre><code>cd $FRCN_ROOT/caffe-fast-rcnn\nmake -j8  多核编译,时间较长\nmake pycaffe\n</code></pre><h4 id=\"4-下载训练好的VGG16和ZF模型\"><a href=\"#4-下载训练好的VGG16和ZF模型\" class=\"headerlink\" title=\"(4)下载训练好的VGG16和ZF模型\"></a>(4)下载训练好的VGG16和ZF模型</h4><pre><code>cd $FRCN_ROOT\n./data/scripts/fetch_faster_rcnn_models.sh\n</code></pre><p>时间太长的话可以考虑找网上别人分享的资源</p>\n<h3 id=\"三-测试源码\"><a href=\"#三-测试源码\" class=\"headerlink\" title=\"(三)测试源码\"></a>(三)测试源码</h3><pre><code>cd $FRCN_ROOT\n./tool/demo.py\n</code></pre><h2 id=\"三-使用faster-rcnn训练自己的数据集\"><a href=\"#三-使用faster-rcnn训练自己的数据集\" class=\"headerlink\" title=\"三. 使用faster-rcnn训练自己的数据集\"></a>三. 使用faster-rcnn训练自己的数据集</h2><h3 id=\"一-下载预训练参数及模型\"><a href=\"#一-下载预训练参数及模型\" class=\"headerlink\" title=\"(一)下载预训练参数及模型\"></a>(一)下载预训练参数及模型</h3><pre><code>cd $FRCN_ROOT\n./data/scripts/fetch_imagenet_models.sh\n./data/scripts/fetch_selective_search_data.sh\n</code></pre><h3 id=\"二-制作数据集\"><a href=\"#二-制作数据集\" class=\"headerlink\" title=\"(二)制作数据集\"></a>(二)制作数据集</h3><p><a href=\"http://report.opsauto.cn/deep%20learning/2017/02/08/%E5%88%B6%E4%BD%9C%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E9%9B%86.html\">制作数据集(VOC2007格式)</a></p>\n<p>将制作好的VOC2007文件夹放置在data/VOCdevkit2007文件夹下,没有则新建VOCdevkit2007文件夹</p>\n<h3 id=\"三-修改配置文件\"><a href=\"#三-修改配置文件\" class=\"headerlink\" title=\"(三)修改配置文件\"></a>(三)修改配置文件</h3><h4 id=\"1-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-fast-rcnn-train-pt和stage2-fast-rcnn-train-pt-两个文件\"><a href=\"#1-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-fast-rcnn-train-pt和stage2-fast-rcnn-train-pt-两个文件\" class=\"headerlink\" title=\"(1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件\"></a>(1)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt和stage2_fast_rcnn_train.pt 两个文件</h4><p>备注:3处修改及其附近的代码</p>\n<pre><code>name: &quot;ZF&quot;\nlayer {\n  name: &apos;data&apos;\n  type: &apos;Python&apos;\n  top: &apos;data&apos;\n  top: &apos;rois&apos;\n  top: &apos;labels&apos;\n  top: &apos;bbox_targets&apos;\n  top: &apos;bbox_inside_weights&apos;\n  top: &apos;bbox_outside_weights&apos;\n  python_param {\n    module: &apos;roi_data_layer.layer&apos;\n    layer: &apos;RoIDataLayer&apos;\n    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1\n  }\n}\n\nlayer {\n  name: &quot;cls_score&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;cls_score&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 2 #按训练集类别改，该值为类别数+1\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.01\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n\nlayer {\n  name: &quot;bbox_pred&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;bbox_pred&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.001\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n</code></pre><h4 id=\"2-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-rpn-train-pt和stage2-rpn-train-pt-两个文件\"><a href=\"#2-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-stage1-rpn-train-pt和stage2-rpn-train-pt-两个文件\" class=\"headerlink\" title=\"(2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件\"></a>(2)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt和stage2_rpn_train.pt 两个文件</h4><p>备注:1处修改及其附近的代码</p>\n<pre><code>layer {\n  name: &apos;input-data&apos;\n  type: &apos;Python&apos;\n  top: &apos;data&apos;\n  top: &apos;im_info&apos;\n  top: &apos;gt_boxes&apos;\n  python_param {\n    module: &apos;roi_data_layer.layer&apos;\n    layer: &apos;RoIDataLayer&apos;\n    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1\n  }\n}\n</code></pre><h4 id=\"3-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-faster-rcnn-test-pt文件\"><a href=\"#3-修改py-faster-rcnn-models-pascal-voc-ZF-faster-rcnn-alt-opt-faster-rcnn-test-pt文件\" class=\"headerlink\" title=\"(3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件\"></a>(3)修改py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt/faster_rcnn_test.pt文件</h4><p>备注:2处修改及其附近的代码</p>\n<pre><code>layer {\n  name: &quot;cls_score&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;cls_score&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 2 #按训练集类别改，该值为类别数+1\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.01\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n\nlayer {\n  name: &quot;bbox_pred&quot;\n  type: &quot;InnerProduct&quot;\n  bottom: &quot;fc7&quot;\n  top: &quot;bbox_pred&quot;\n  param { lr_mult: 1.0 }\n  param { lr_mult: 2.0 }\n  inner_product_param {\n    num_output: 8 #按训练集类别改，该值为（类别数+1）*4\n    weight_filler {\n      type: &quot;gaussian&quot;\n      std: 0.001\n    }\n    bias_filler {\n      type: &quot;constant&quot;\n      value: 0\n    }\n  }\n}\n</code></pre><h4 id=\"4-修改py-faster-rcnn-lib-datasets-pascal-voc-py\"><a href=\"#4-修改py-faster-rcnn-lib-datasets-pascal-voc-py\" class=\"headerlink\" title=\"(4)修改py-faster-rcnn/lib/datasets/pascal_voc.py\"></a>(4)修改py-faster-rcnn/lib/datasets/pascal_voc.py</h4><pre><code>self._classes = (&apos;__background__&apos;, # always index 0\n                         &apos;你的标签1&apos;,&apos;你的标签2&apos;,你的标签3&apos;,&apos;你的标签4&apos;)\n\n注:如果只是在原始检测的20种类别:&apos;aeroplane&apos;, &apos;bicycle&apos;, &apos;bird&apos;, &apos;boat&apos;,&apos;bottle&apos;, &apos;bus&apos;, &apos;car&apos;, &apos;cat&apos;, &apos;chair&apos;,\n&apos;cow&apos;, &apos;diningtable&apos;, &apos;dog&apos;, &apos;horse&apos;,&apos;motorbike&apos;, &apos;person&apos;, &apos;pottedplant&apos;,\n&apos;sheep&apos;, &apos;sofa&apos;, &apos;train&apos;, &apos;tvmonitor&apos;中检测单一类别,可参考修改下面的代码:\n\n\ndef _load_image_set_index(self):\n        &quot;&quot;&quot;\n        Load the indexes listed in this dataset&apos;s image set file.\n        &quot;&quot;&quot;\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, &apos;ImageSets&apos;, &apos;Main&apos;,\n                                      self._image_set + &apos;.txt&apos;)\n        assert os.path.exists(image_set_file), \\\n                &apos;Path does not exist: {}&apos;.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n\n注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n        # only load index with cars obj\n        new_image_index = []\n        for index in image_index:\n            filename = os.path.join(self._data_path, &apos;Annotations&apos;, index + &apos;.xml&apos;)\n            tree = ET.parse(filename)\n            objs = tree.findall(&apos;object&apos;)\n            num_objs = 0\n            for ix, obj in enumerate(objs):\n                curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n                if curr_name == &apos;car&apos;:\n                    num_objs += 1\n                    break\n            if num_objs &gt; 0:\n                new_image_index.append(index)\n        return new_image_index\n\ndef _load_pascal_annotation(self, index):\n        &quot;&quot;&quot;\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        &quot;&quot;&quot;\n        filename = os.path.join(self._data_path, &apos;Annotations&apos;, index + &apos;.xml&apos;)\n        tree = ET.parse(filename)\n        objs = tree.findall(&apos;object&apos;)\n        if not self.config[&apos;use_diff&apos;]:\n            # Exclude the samples labeled as difficult\n            non_diff_objs = [\n                obj for obj in objs if int(obj.find(&apos;difficult&apos;).text) == 0]\n            # if len(non_diff_objs) != len(objs):\n            #     print &apos;Removed {} difficult objects&apos;.format(\n            #         len(objs) - len(non_diff_objs))\n            objs = non_diff_objs\n\n注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码.\n        # change num objs , only read car\n        # num_objs = len(objs)\n\n        num_objs = 0\n        for ix, obj in enumerate(objs):\n            curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n            if curr_name == &apos;car&apos;:\n                num_objs += 1\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # &quot;Seg&quot; area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n#注:如果需要在原始的20类别只检测车辆的话才需要修改这部分代码\n# Load object bounding boxes into a data     frame.\n        tmp_ix = 0\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(&apos;bndbox&apos;)\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(&apos;xmin&apos;).text) - 1\n            y1 = float(bbox.find(&apos;ymin&apos;).text) - 1\n            x2 = float(bbox.find(&apos;xmax&apos;).text) - 1\n            y2 = float(bbox.find(&apos;ymax&apos;).text) - 1\n            curr_name = obj.find(&apos;name&apos;).text.lower().strip()\n            if curr_name != &apos;car&apos;:\n                continue\n            cls = self._class_to_ind[curr_name]\n            boxes[tmp_ix, :] = [x1, y1, x2, y2]\n            gt_classes[tmp_ix] = cls\n            overlaps[tmp_ix, cls] = 1.0\n            seg_areas[tmp_ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n            tmp_ix += 1\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {&apos;boxes&apos; : boxes,\n                &apos;gt_classes&apos;: gt_classes,\n                &apos;gt_overlaps&apos; : overlaps,\n                &apos;flipped&apos; : False,\n                &apos;seg_areas&apos; : seg_areas}\n</code></pre><h4 id=\"4-py-faster-rcnn-lib-datasets-imdb-py修改\"><a href=\"#4-py-faster-rcnn-lib-datasets-imdb-py修改\" class=\"headerlink\" title=\"(4)py-faster-rcnn/lib/datasets/imdb.py修改\"></a>(4)py-faster-rcnn/lib/datasets/imdb.py修改</h4><pre><code>def append_flipped_images(self):\n        num_images = self.num_images\n        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n                  for i in xrange(num_images)]\n        for i in xrange(num_images):\n            boxes = self.roidb[i][&apos;boxes&apos;].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n\n            for b in range(len(boxes)):\n                if boxes[b][2] &lt; boxes[b][0]:\n                   boxes[b][0] = 0\n\n            assert (boxes[:, 2] &gt;= boxes[:, 0]).all()\n</code></pre><h4 id=\"5-py-faster-rcnn-tools-train-faster-rcnn-alt-opt-py修改迭代次数（建议修改）\"><a href=\"#5-py-faster-rcnn-tools-train-faster-rcnn-alt-opt-py修改迭代次数（建议修改）\" class=\"headerlink\" title=\"(5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）\"></a>(5)py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py修改迭代次数（建议修改）</h4><pre><code>max_iters=[8000,4000,8000,4000]\n建议:第一次训练使用较低的迭代次数,先确保能正常训练,如max_iters=[8,4,8,4]\n</code></pre><p>训练分别为4个阶段（rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段）的迭代次数。可改成你希望的迭代次数。<br>如果改了这些数值，最好把py-faster-rcnn/models/pascal_voc/ZF/faster_rcnn_alt_opt里对应的solver文件（有4个）也修改，stepsize小于上面修改的数值，stepsize的意义是经过stepsize次的迭代后降低一次学习率（非必要修改）。</p>\n<h4 id=\"6-删除缓存文件-每次修改配置文件后训练都要做\"><a href=\"#6-删除缓存文件-每次修改配置文件后训练都要做\" class=\"headerlink\" title=\"(6)删除缓存文件(每次修改配置文件后训练都要做)\"></a>(6)删除缓存文件(每次修改配置文件后训练都要做)</h4><pre><code>删除py-faster-rcnn文件夹下所有的.pyc文件及data文件夹下的cache文件夹,\ndata/VOCdekit2007下的annotations_cache文件夹(最近一次成功训练的\nannotation和当前annotation一样的话这部分可以不删,否则可以正常训练,\n但是最后评价模型会出错)\n</code></pre><h3 id=\"四-开始训练\"><a href=\"#四-开始训练\" class=\"headerlink\" title=\"(四)开始训练\"></a>(四)开始训练</h3><pre><code>cd $FRCN_ROOT\n./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc\n</code></pre><p>成功训练后在py-faster-rcnn/output/faster_rcnn_alt_opt/voc_2007_trainval文件夹下<br>会有以final.caffemodel结尾的模型文件,一般为ZF_faster_rcnn_final.caffemodel</p>\n<p>成功训练后会有一次模型性能的评估测试,成功的话会有MAP指标和平均MAP指标的输出,类似下文,<br>训练日志文件保存在experiments/logs文件夹下.</p>\n<pre><code>Evaluating detections\nWriting car VOC results file\nVOC07 metric? Yes\nAP for car = 0.0090\nMean AP = 0.0090\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">Results:</div><div class=\"line\">0.009</div><div class=\"line\">0.009</div></pre></td></tr></table></figure>\n\n\n--------------------------------------------------------------\nResults computed with the **unofficial** Python eval code.\nResults should be very close to the official MATLAB eval code.\nRecompute with `./tools/reval.py --matlab ...` for your paper.\n-- Thanks, The Management\n--------------------------------------------------------------\n\nreal    1m43.822s\nuser    1m25.764s\nsys    0m15.516s\n</code></pre><h3 id=\"五-测试训练结果\"><a href=\"#五-测试训练结果\" class=\"headerlink\" title=\"(五)测试训练结果\"></a>(五)测试训练结果</h3><h4 id=\"1-修改py-faster-rcnn-tools-demo-py\"><a href=\"#1-修改py-faster-rcnn-tools-demo-py\" class=\"headerlink\" title=\"(1)修改py-faster-rcnn\\tools\\demo.py\"></a>(1)修改py-faster-rcnn\\tools\\demo.py</h4><pre><code>CLASSES = (&apos;__background__&apos;,\n         &apos;你的标签1&apos;,&apos;你的标签2&apos;,你的标签3&apos;,&apos;你的标签4&apos;)\n\nNETS = {&apos;vgg16&apos;: (&apos;VGG16&apos;,\n                  &apos;VGG16_faster_rcnn_final.caffemodel&apos;),\n        &apos;zf&apos;: (&apos;ZF&apos;,\n                  &apos;ZF_faster_rcnn_final.caffemodel&apos;)}\n\nim_names = os.listdir(os.path.join(cfg.DATA_DIR, &apos;demo&apos;))  \n</code></pre><h4 id=\"2-放置模型及测试图片\"><a href=\"#2-放置模型及测试图片\" class=\"headerlink\" title=\"(2)放置模型及测试图片\"></a>(2)放置模型及测试图片</h4><pre><code>将训练得到的py-faster-rcnn\\output\\faster_rcnn_alt_opt\\***_trainval中\nZF的final.caffemodel拷贝至py-faster-rcnn\\data\\faster_rcnn_models\n\n测试图片放在py-faster-rcnn\\data\\demo(与上面demo.py设置路径有关,可修改)\n</code></pre><h4 id=\"3-进行测试\"><a href=\"#3-进行测试\" class=\"headerlink\" title=\"(3)进行测试\"></a>(3)进行测试</h4><pre><code>cd $FRCN_ROOT\n./tool/demo.py\n</code></pre><h2 id=\"四-曾出现过的bug及当时的解决方法\"><a href=\"#四-曾出现过的bug及当时的解决方法\" class=\"headerlink\" title=\"四. 曾出现过的bug及当时的解决方法\"></a>四. 曾出现过的bug及当时的解决方法</h2><p>(1) 训练时出现KeyError:’max_overlaps’  ,解决方法:删除data文件夹下的cache文件夹</p>\n<p>(2) 训练结束后测试时出现类似</p>\n<pre><code>File &quot;/home/hyzhan/py-faster-rcnn/tools/../lib/datasets/voc_eval.py&quot;, line 126, in voc_eval\n    R = [obj for obj in recs[imagename] if obj[&apos;name&apos;] == classname]\nKeyError: &apos;000002&apos;\n</code></pre><p>解决方法: 删除data/VOCdekit2007下的annotations_cache文件夹</p>\n<p>(3) caffe-fast-rcnn编译时出现找不到nvcc命令的情况,解决方法:</p>\n<pre><code>export PATH=/usr/local/cuda-8.0/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH\n</code></pre><p>将cuda安装路径添加到环境变量中</p>\n<p>(4) caffe-fast-rcnn编译时出现类似找不到opencv命令的情况,解决方法,添加环境变量:</p>\n<pre><code>export LD_LIBRARY_PATH=/home/hyzhan/software/opencv3/lib:$LD_LIBRARY_PATH\n</code></pre><p>(5) 训练的时候执行”./experiments/scripts/faster_rcnn_alt_opt.sh 0 ZF pascal_voc”语句进行训练会出现找不到faster_rcnn_alt_opt.sh文件的情况,解决方法:重新手打命令</p>\n<p>(6) 测试之前需要修改tool文件夹下的demo或者mydemo里面的class类别,不然会显示上次训练的类别</p>"},{"layout":"post","title":"用YOLOv2训练自己的数据集","date":"2017-02-07T09:42:00.000Z","description":"用YOLOv2训练自己的数据集","_content":"\n\n## 一. 系统初始环境\n\n<!--more-->\n\n**系统:Ubuntu16.04**:  ubuntu-16.04-desktop-amd64.iso<br />\n\n**cuda安装文件**: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接[点击](https://developer.nvidia.com/cuda-downloads), linux-x86架构-ubuntu-16.04-deb(local) <br />\n**cudnn安装文件**: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接[点击](https://developer.nvidia.com/cudnn), 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大 <br />\n**caffe源代码**: [github链接](https://github.com/BVLC/caffe) ,或者运行git clone https://github.com/BVLC/caffe.git <br />\n\n**安装过程**：[点击这里](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n\n## 二. 初始化YOLO(v2版本)\n\n1. git clone https://github.com/pjreddie/darknet\n2. cd darknet\n3. make\n注意默认Markfile是没有开启GPU模式的，需要适当修改：\nGPU=1\nCUDNN=1\nOPENCV=1\n\n开启OPENCV=1后编译可能出现找不到lippicv的情况，笔者是将/usr/local/share/OpenCV/3rdparty/lib 里面的libippicv.a文件复制到/usr/lib解决的，因为使用OPENCV=0后编译后训练时无法通过stb_image.h导入图像\n\n\n## 三. 测试YOLO\n\n1. 下载训练好的权重 wget http://pjreddie.com/media/files/yolo.weights\n2. 进行测试 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg\n\n## 三. 训练自己的数据\n\n#### (一).准备数据集\n\n1. 制作数据集，将图片制作成VOC2007格式\n2. 把数据集放在darknet/script中，数据集结构为VOCdevkit/VOC2007/*，如果需要改动数据集位置或名称，需要同时修改voc_label.py里面对应的文件路径 \n3. 修改voc_label.py中的classes变成自己需要的类别名称\n4. 运行voc_label.py\n5. 可选 sudo cat 2007_train.txt 2007_val.txt > train.txt\n\n#### (二).修改配置\n\n1. 修改data/voc.names里面的类别为自己需要的类别名称\n2. 修改cfg/voc.data文件\n> classes= 20\n\n> train  = 对应路径/train.txt\n\n> valid  = 对应路径/2007_test.txt\n\n> names = data/voc.names\n\n> backup = 对应路径/backup\n\n3. 修改src/yolo_kernel.cu文件\n> 约行62：draw_detections(det, l.side*l.side*l.n, demo_thresh, boxes, probs, voc_names, voc_labels, 你的类别数);\n\n4. cfg/tiny-yolo-voc.cfg文件（也可复制重命名为其他，如my.cfg）\n> 学习率 learning_rate=0.001\n\n> 最大迭代次数 max_batches = 1001（测试时可以调低一些，100次迭代需要8分钟左右，迭代次数太少训练出来的权重可能看不到测试效果）\n\n[convolutional]\n\nsize=1\n\nstride=1\n\npad=1\n\nfilters=35  #filters=（classes+coords+1）*5\n\nactivation=linear\n\n[region]\n\nanchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52\n\nbias_match=1\n\nclasses=2  #你的类别数\n\ncoords=4\n\nnum=5\n\nsoftmax=1\n\njitter=.2\n\nrescore=1\n\n\n#### (三).训练\n\n1. 下载预训练模型 \n>curl -O http://pjreddie.com/media/files/darknet19_448.conv.23\n2. 开始训练\n> ./darknet detector train cfg/voc.data cfg/my.cfg darknet19_448.conv.23\n\n## 四. 测试训练模型效果\n\n> ./darknet detector test cfg/voc.data cfg/my.cfg 你的backup目录/my_final.weights data/测试图片.jpg\n命令行后接-thresh 0.05可调整置信度阈值","source":"_posts/2017-02-07-用YOLOv2训练自己的数据集.md","raw":"---\nlayout: post\ntitle: \"用YOLOv2训练自己的数据集\"\ndate: 2017-02-07 17:42:00\ndescription: \"用YOLOv2训练自己的数据集\"\ncategory: [deep learning]\ntags: [YOLOv2]\n---\n\n\n## 一. 系统初始环境\n\n<!--more-->\n\n**系统:Ubuntu16.04**:  ubuntu-16.04-desktop-amd64.iso<br />\n\n**cuda安装文件**: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接[点击](https://developer.nvidia.com/cuda-downloads), linux-x86架构-ubuntu-16.04-deb(local) <br />\n**cudnn安装文件**: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接[点击](https://developer.nvidia.com/cudnn), 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大 <br />\n**caffe源代码**: [github链接](https://github.com/BVLC/caffe) ,或者运行git clone https://github.com/BVLC/caffe.git <br />\n\n**安装过程**：[点击这里](http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html)\n\n## 二. 初始化YOLO(v2版本)\n\n1. git clone https://github.com/pjreddie/darknet\n2. cd darknet\n3. make\n注意默认Markfile是没有开启GPU模式的，需要适当修改：\nGPU=1\nCUDNN=1\nOPENCV=1\n\n开启OPENCV=1后编译可能出现找不到lippicv的情况，笔者是将/usr/local/share/OpenCV/3rdparty/lib 里面的libippicv.a文件复制到/usr/lib解决的，因为使用OPENCV=0后编译后训练时无法通过stb_image.h导入图像\n\n\n## 三. 测试YOLO\n\n1. 下载训练好的权重 wget http://pjreddie.com/media/files/yolo.weights\n2. 进行测试 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg\n\n## 三. 训练自己的数据\n\n#### (一).准备数据集\n\n1. 制作数据集，将图片制作成VOC2007格式\n2. 把数据集放在darknet/script中，数据集结构为VOCdevkit/VOC2007/*，如果需要改动数据集位置或名称，需要同时修改voc_label.py里面对应的文件路径 \n3. 修改voc_label.py中的classes变成自己需要的类别名称\n4. 运行voc_label.py\n5. 可选 sudo cat 2007_train.txt 2007_val.txt > train.txt\n\n#### (二).修改配置\n\n1. 修改data/voc.names里面的类别为自己需要的类别名称\n2. 修改cfg/voc.data文件\n> classes= 20\n\n> train  = 对应路径/train.txt\n\n> valid  = 对应路径/2007_test.txt\n\n> names = data/voc.names\n\n> backup = 对应路径/backup\n\n3. 修改src/yolo_kernel.cu文件\n> 约行62：draw_detections(det, l.side*l.side*l.n, demo_thresh, boxes, probs, voc_names, voc_labels, 你的类别数);\n\n4. cfg/tiny-yolo-voc.cfg文件（也可复制重命名为其他，如my.cfg）\n> 学习率 learning_rate=0.001\n\n> 最大迭代次数 max_batches = 1001（测试时可以调低一些，100次迭代需要8分钟左右，迭代次数太少训练出来的权重可能看不到测试效果）\n\n[convolutional]\n\nsize=1\n\nstride=1\n\npad=1\n\nfilters=35  #filters=（classes+coords+1）*5\n\nactivation=linear\n\n[region]\n\nanchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52\n\nbias_match=1\n\nclasses=2  #你的类别数\n\ncoords=4\n\nnum=5\n\nsoftmax=1\n\njitter=.2\n\nrescore=1\n\n\n#### (三).训练\n\n1. 下载预训练模型 \n>curl -O http://pjreddie.com/media/files/darknet19_448.conv.23\n2. 开始训练\n> ./darknet detector train cfg/voc.data cfg/my.cfg darknet19_448.conv.23\n\n## 四. 测试训练模型效果\n\n> ./darknet detector test cfg/voc.data cfg/my.cfg 你的backup目录/my_final.weights data/测试图片.jpg\n命令行后接-thresh 0.05可调整置信度阈值","slug":"2017-02-07-用YOLOv2训练自己的数据集","published":1,"updated":"2017-04-18T05:45:39.187Z","comments":1,"photos":[],"link":"","_id":"cj2ycuxj60007tdjcwx1a2gor","content":"<h2 id=\"一-系统初始环境\"><a href=\"#一-系统初始环境\" class=\"headerlink\" title=\"一. 系统初始环境\"></a>一. 系统初始环境</h2><a id=\"more\"></a>\n<p><strong>系统:Ubuntu16.04</strong>:  ubuntu-16.04-desktop-amd64.iso<br></p>\n<p><strong>cuda安装文件</strong>: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接<a href=\"https://developer.nvidia.com/cuda-downloads\" target=\"_blank\" rel=\"external\">点击</a>, linux-x86架构-ubuntu-16.04-deb(local) <br><br><strong>cudnn安装文件</strong>: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接<a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\" rel=\"external\">点击</a>, 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大 <br><br><strong>caffe源代码</strong>: <a href=\"https://github.com/BVLC/caffe\" target=\"_blank\" rel=\"external\">github链接</a> ,或者运行git clone <a href=\"https://github.com/BVLC/caffe.git\" target=\"_blank\" rel=\"external\">https://github.com/BVLC/caffe.git</a> <br></p>\n<p><strong>安装过程</strong>：<a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\" target=\"_blank\" rel=\"external\">点击这里</a></p>\n<h2 id=\"二-初始化YOLO-v2版本\"><a href=\"#二-初始化YOLO-v2版本\" class=\"headerlink\" title=\"二. 初始化YOLO(v2版本)\"></a>二. 初始化YOLO(v2版本)</h2><ol>\n<li>git clone <a href=\"https://github.com/pjreddie/darknet\" target=\"_blank\" rel=\"external\">https://github.com/pjreddie/darknet</a></li>\n<li>cd darknet</li>\n<li>make<br>注意默认Markfile是没有开启GPU模式的，需要适当修改：<br>GPU=1<br>CUDNN=1<br>OPENCV=1</li>\n</ol>\n<p>开启OPENCV=1后编译可能出现找不到lippicv的情况，笔者是将/usr/local/share/OpenCV/3rdparty/lib 里面的libippicv.a文件复制到/usr/lib解决的，因为使用OPENCV=0后编译后训练时无法通过stb_image.h导入图像</p>\n<h2 id=\"三-测试YOLO\"><a href=\"#三-测试YOLO\" class=\"headerlink\" title=\"三. 测试YOLO\"></a>三. 测试YOLO</h2><ol>\n<li>下载训练好的权重 wget <a href=\"http://pjreddie.com/media/files/yolo.weights\" target=\"_blank\" rel=\"external\">http://pjreddie.com/media/files/yolo.weights</a></li>\n<li>进行测试 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg</li>\n</ol>\n<h2 id=\"三-训练自己的数据\"><a href=\"#三-训练自己的数据\" class=\"headerlink\" title=\"三. 训练自己的数据\"></a>三. 训练自己的数据</h2><h4 id=\"一-准备数据集\"><a href=\"#一-准备数据集\" class=\"headerlink\" title=\"(一).准备数据集\"></a>(一).准备数据集</h4><ol>\n<li>制作数据集，将图片制作成VOC2007格式</li>\n<li>把数据集放在darknet/script中，数据集结构为VOCdevkit/VOC2007/*，如果需要改动数据集位置或名称，需要同时修改voc_label.py里面对应的文件路径 </li>\n<li>修改voc_label.py中的classes变成自己需要的类别名称</li>\n<li>运行voc_label.py</li>\n<li>可选 sudo cat 2007_train.txt 2007_val.txt &gt; train.txt</li>\n</ol>\n<h4 id=\"二-修改配置\"><a href=\"#二-修改配置\" class=\"headerlink\" title=\"(二).修改配置\"></a>(二).修改配置</h4><ol>\n<li>修改data/voc.names里面的类别为自己需要的类别名称</li>\n<li>修改cfg/voc.data文件<blockquote>\n<p>classes= 20</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>train  = 对应路径/train.txt</p>\n<p>valid  = 对应路径/2007_test.txt</p>\n<p>names = data/voc.names</p>\n<p>backup = 对应路径/backup</p>\n</blockquote>\n<ol>\n<li><p>修改src/yolo_kernel.cu文件</p>\n<blockquote>\n<p>约行62：draw_detections(det, l.side<em>l.side</em>l.n, demo_thresh, boxes, probs, voc_names, voc_labels, 你的类别数);</p>\n</blockquote>\n</li>\n<li><p>cfg/tiny-yolo-voc.cfg文件（也可复制重命名为其他，如my.cfg）</p>\n<blockquote>\n<p>学习率 learning_rate=0.001</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>最大迭代次数 max_batches = 1001（测试时可以调低一些，100次迭代需要8分钟左右，迭代次数太少训练出来的权重可能看不到测试效果）</p>\n</blockquote>\n<p>[convolutional]</p>\n<p>size=1</p>\n<p>stride=1</p>\n<p>pad=1</p>\n<p>filters=35  #filters=（classes+coords+1）*5</p>\n<p>activation=linear</p>\n<p>[region]</p>\n<p>anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52</p>\n<p>bias_match=1</p>\n<p>classes=2  #你的类别数</p>\n<p>coords=4</p>\n<p>num=5</p>\n<p>softmax=1</p>\n<p>jitter=.2</p>\n<p>rescore=1</p>\n<h4 id=\"三-训练\"><a href=\"#三-训练\" class=\"headerlink\" title=\"(三).训练\"></a>(三).训练</h4><ol>\n<li>下载预训练模型 <blockquote>\n<p>curl -O <a href=\"http://pjreddie.com/media/files/darknet19_448.conv.23\" target=\"_blank\" rel=\"external\">http://pjreddie.com/media/files/darknet19_448.conv.23</a></p>\n</blockquote>\n</li>\n<li>开始训练<blockquote>\n<p>./darknet detector train cfg/voc.data cfg/my.cfg darknet19_448.conv.23</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"四-测试训练模型效果\"><a href=\"#四-测试训练模型效果\" class=\"headerlink\" title=\"四. 测试训练模型效果\"></a>四. 测试训练模型效果</h2><blockquote>\n<p>./darknet detector test cfg/voc.data cfg/my.cfg 你的backup目录/my_final.weights data/测试图片.jpg<br>命令行后接-thresh 0.05可调整置信度阈值</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"一-系统初始环境\"><a href=\"#一-系统初始环境\" class=\"headerlink\" title=\"一. 系统初始环境\"></a>一. 系统初始环境</h2>","more":"<p><strong>系统:Ubuntu16.04</strong>:  ubuntu-16.04-desktop-amd64.iso<br /></p>\n<p><strong>cuda安装文件</strong>: cuda-repo-ubuntu1604-8-0-local_8.0.44-1_amd64.deb.44-1_amd64-deb,下载链接<a href=\"https://developer.nvidia.com/cuda-downloads\">点击</a>, linux-x86架构-ubuntu-16.04-deb(local) <br /><br><strong>cudnn安装文件</strong>: cudnn-8.0-linux-x64-v5.0-ga.solitairetheme8,下载链接<a href=\"https://developer.nvidia.com/cudnn\">点击</a>, 适用cuda8.0有5.1和5.0版,这里用5.0版,区别应该不大 <br /><br><strong>caffe源代码</strong>: <a href=\"https://github.com/BVLC/caffe\">github链接</a> ,或者运行git clone <a href=\"https://github.com/BVLC/caffe.git\">https://github.com/BVLC/caffe.git</a> <br /></p>\n<p><strong>安装过程</strong>：<a href=\"http://report.opsauto.cn/deep%20learning/2016/11/12/ubunt16.04%E4%B8%8Bcaffe%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85.html\">点击这里</a></p>\n<h2 id=\"二-初始化YOLO-v2版本\"><a href=\"#二-初始化YOLO-v2版本\" class=\"headerlink\" title=\"二. 初始化YOLO(v2版本)\"></a>二. 初始化YOLO(v2版本)</h2><ol>\n<li>git clone <a href=\"https://github.com/pjreddie/darknet\">https://github.com/pjreddie/darknet</a></li>\n<li>cd darknet</li>\n<li>make<br>注意默认Markfile是没有开启GPU模式的，需要适当修改：<br>GPU=1<br>CUDNN=1<br>OPENCV=1</li>\n</ol>\n<p>开启OPENCV=1后编译可能出现找不到lippicv的情况，笔者是将/usr/local/share/OpenCV/3rdparty/lib 里面的libippicv.a文件复制到/usr/lib解决的，因为使用OPENCV=0后编译后训练时无法通过stb_image.h导入图像</p>\n<h2 id=\"三-测试YOLO\"><a href=\"#三-测试YOLO\" class=\"headerlink\" title=\"三. 测试YOLO\"></a>三. 测试YOLO</h2><ol>\n<li>下载训练好的权重 wget <a href=\"http://pjreddie.com/media/files/yolo.weights\">http://pjreddie.com/media/files/yolo.weights</a></li>\n<li>进行测试 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg</li>\n</ol>\n<h2 id=\"三-训练自己的数据\"><a href=\"#三-训练自己的数据\" class=\"headerlink\" title=\"三. 训练自己的数据\"></a>三. 训练自己的数据</h2><h4 id=\"一-准备数据集\"><a href=\"#一-准备数据集\" class=\"headerlink\" title=\"(一).准备数据集\"></a>(一).准备数据集</h4><ol>\n<li>制作数据集，将图片制作成VOC2007格式</li>\n<li>把数据集放在darknet/script中，数据集结构为VOCdevkit/VOC2007/*，如果需要改动数据集位置或名称，需要同时修改voc_label.py里面对应的文件路径 </li>\n<li>修改voc_label.py中的classes变成自己需要的类别名称</li>\n<li>运行voc_label.py</li>\n<li>可选 sudo cat 2007_train.txt 2007_val.txt &gt; train.txt</li>\n</ol>\n<h4 id=\"二-修改配置\"><a href=\"#二-修改配置\" class=\"headerlink\" title=\"(二).修改配置\"></a>(二).修改配置</h4><ol>\n<li>修改data/voc.names里面的类别为自己需要的类别名称</li>\n<li>修改cfg/voc.data文件<blockquote>\n<p>classes= 20</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>train  = 对应路径/train.txt</p>\n<p>valid  = 对应路径/2007_test.txt</p>\n<p>names = data/voc.names</p>\n<p>backup = 对应路径/backup</p>\n</blockquote>\n<ol>\n<li><p>修改src/yolo_kernel.cu文件</p>\n<blockquote>\n<p>约行62：draw_detections(det, l.side<em>l.side</em>l.n, demo_thresh, boxes, probs, voc_names, voc_labels, 你的类别数);</p>\n</blockquote>\n</li>\n<li><p>cfg/tiny-yolo-voc.cfg文件（也可复制重命名为其他，如my.cfg）</p>\n<blockquote>\n<p>学习率 learning_rate=0.001</p>\n</blockquote>\n</li>\n</ol>\n<blockquote>\n<p>最大迭代次数 max_batches = 1001（测试时可以调低一些，100次迭代需要8分钟左右，迭代次数太少训练出来的权重可能看不到测试效果）</p>\n</blockquote>\n<p>[convolutional]</p>\n<p>size=1</p>\n<p>stride=1</p>\n<p>pad=1</p>\n<p>filters=35  #filters=（classes+coords+1）*5</p>\n<p>activation=linear</p>\n<p>[region]</p>\n<p>anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52</p>\n<p>bias_match=1</p>\n<p>classes=2  #你的类别数</p>\n<p>coords=4</p>\n<p>num=5</p>\n<p>softmax=1</p>\n<p>jitter=.2</p>\n<p>rescore=1</p>\n<h4 id=\"三-训练\"><a href=\"#三-训练\" class=\"headerlink\" title=\"(三).训练\"></a>(三).训练</h4><ol>\n<li>下载预训练模型 <blockquote>\n<p>curl -O <a href=\"http://pjreddie.com/media/files/darknet19_448.conv.23\">http://pjreddie.com/media/files/darknet19_448.conv.23</a></p>\n</blockquote>\n</li>\n<li>开始训练<blockquote>\n<p>./darknet detector train cfg/voc.data cfg/my.cfg darknet19_448.conv.23</p>\n</blockquote>\n</li>\n</ol>\n<h2 id=\"四-测试训练模型效果\"><a href=\"#四-测试训练模型效果\" class=\"headerlink\" title=\"四. 测试训练模型效果\"></a>四. 测试训练模型效果</h2><blockquote>\n<p>./darknet detector test cfg/voc.data cfg/my.cfg 你的backup目录/my_final.weights data/测试图片.jpg<br>命令行后接-thresh 0.05可调整置信度阈值</p>\n</blockquote>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj2ycuxj60007tdjcwx1a2gor","category_id":"cj2ycuxi50004tdjcbmmct09y","_id":"cj2ycuxka000btdjcggyxh0ks"},{"post_id":"cj2ycuxgx0000tdjcdz8922ze","category_id":"cj2ycuxi50004tdjcbmmct09y","_id":"cj2ycuxko000etdjc792s6ig3"},{"post_id":"cj2ycuxhx0002tdjcb9ljqff1","category_id":"cj2ycuxi50004tdjcbmmct09y","_id":"cj2ycuxkp000gtdjcm4k1sbb3"},{"post_id":"cj2ycuxiy0006tdjc7kpm2in9","category_id":"cj2ycuxi50004tdjcbmmct09y","_id":"cj2ycuxl0000jtdjcmndegqyr"}],"PostTag":[{"post_id":"cj2ycuxgx0000tdjcdz8922ze","tag_id":"cj2ycuxix0005tdjcv1xksvwd","_id":"cj2ycuxk2000atdjch80mfr62"},{"post_id":"cj2ycuxhx0002tdjcb9ljqff1","tag_id":"cj2ycuxjt0009tdjcx2hq08nh","_id":"cj2ycuxko000ftdjcjzjxf3gj"},{"post_id":"cj2ycuxiy0006tdjc7kpm2in9","tag_id":"cj2ycuxjt0009tdjcx2hq08nh","_id":"cj2ycuxkz000itdjcltmtks6j"},{"post_id":"cj2ycuxj60007tdjcwx1a2gor","tag_id":"cj2ycuxkq000htdjcqvfn5xav","_id":"cj2ycuxl0000ktdjcgir98ny7"}],"Tag":[{"name":"caffe","_id":"cj2ycuxix0005tdjcv1xksvwd"},{"name":"python","_id":"cj2ycuxjt0009tdjcx2hq08nh"},{"name":"YOLOv2","_id":"cj2ycuxkq000htdjcqvfn5xav"}]}}